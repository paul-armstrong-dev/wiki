
Comparison sorts
----

| Name     |        Best     |        Average     |        Worst     |        Memory     |        Stable     |        Method     |        Other notes     |
|----------|-----------------|--------------------|------------------|-------------------|-------------------|-------------------|------------------------|
| Quicksort     |        {\displaystyle n\log n}n\log n     |        | variation is n     |        {\displaystyle n\log n}n\log n     |        {\displaystyle n^{2}}n^{2}     |        {\displaystyle \log n}\log n on average, worst case space complexity is n; Sedgewick variation is {\displaystyle \log n}\log n worst case.     |        Typical in-place sort is not stable; stable versions exist.     |        Partitioning     |        Quicksort is usually done in-place with O(log n) stack space.[5][6]     |        
| Merge sort     |        {\displaystyle n\log n}n\log n     |        {\displaystyle n\log n}n\log n     |        {\displaystyle n\log n}n\log n     |        n     | A hybrid block merge sort is O(1) mem.     |        Yes     |        Merging     |        Highly parallelizable (up to O(log n) using the Three Hungarians' Algorithm[7] or, more practically, Cole's parallel merge sort) for processing large amounts of data.     |        
| In-place merge sort     |        —     |        —     |        {\displaystyle n\log ^{2}n}n\log ^{2}n     | See above, for hybrid, that is {\displaystyle n\log n}n\log n     |        1     |        Yes     |        Merging     |        Can be implemented as a stable sort based on stable in-place merging.[8]     |        
| Heapsort     |        n     | If all keys are distinct, {\displaystyle n\log n}n\log n     |        {\displaystyle n\log n}n\log n     |        {\displaystyle n\log n}n\log n     |        1     |        No     |        Selection     |             |        
| Insertion sort     |        n     |        {\displaystyle n^{2}}n^{2}     |        {\displaystyle n^{2}}n^{2}     |        1     |        Yes     |        Insertion     |        O(n + d), in the worst case over sequences that have d inversions.     |        
| Introsort     |        {\displaystyle n\log n}n\log n     |        {\displaystyle n\log n}n\log n     |        {\displaystyle n\log n}n\log n     |        {\displaystyle \log n}\log n     |        No     |        Partitioning & Selection     |        Used in several STL implementations.     |        
| Selection sort     |        {\displaystyle n^{2}}n^{2}     |        {\displaystyle n^{2}}n^{2}     |        {\displaystyle n^{2}}n^{2}     |        1     |        No     |        Selection     |        Stable with {\displaystyle O(n)}O(n) extra space or when using linked lists.[9]     |        
| Timsort     |        n     |        {\displaystyle n\log n}n\log n     |        {\displaystyle n\log n}n\log n     |        n     |        Yes     |        Insertion & Merging     |        Makes n comparisons when the data is already sorted or reverse sorted.     |        
| Cubesort     |        n     |        {\displaystyle n\log n}n\log n     |        {\displaystyle n\log n}n\log n     |        n     |        Yes     |        Insertion     |        Makes n comparisons when the data is already sorted or reverse sorted.     |        
| Shell sort     |        {\displaystyle n\log n}n\log n     |        Depends on gap sequence     |        Depends on gap sequence;     | best known is {\displaystyle n^{4/3}}{\displaystyle n^{4/3}}     |        1     |        No     |        Insertion     |        Small code size, no use of call stack, reasonably fast, useful where memory is at a premium such as embedded and older mainframe applications. There is a worst case {\displaystyle O(n(\log n)^{2})}{\displaystyle O(n(\log n)^{2})} gap sequence but it loses {\displaystyle O(n\log n)}O(n\log n) best case time.     |        
| Bubble sort     |        n     |        {\displaystyle n^{2}}n^{2}     |        {\displaystyle n^{2}}n^{2}     |        1     |        Yes     |        Exchanging     |        Tiny code size.     |        
| Binary tree sort     |        {\displaystyle n\log n}n\log n     |        {\displaystyle n\log n}n\log n     |        {\displaystyle n\log n}n\log n(balanced)     |        n     |        Yes     |        Insertion     |        When using a self-balancing binary search tree.     |        
| Cycle sort     |        {\displaystyle n^{2}}n^{2}     |        {\displaystyle n^{2}}n^{2}     |        {\displaystyle n^{2}}n^{2}     |        1     |        No     |        Insertion     |        In-place with theoretically optimal number of writes.     |        
| Library sort     |        n     |        {\displaystyle n\log n}n\log n     |        {\displaystyle n^{2}}n^{2}     |        n     |        Yes     |        Insertion     |             |        
| Patience sorting     |        n     |        —     |        {\displaystyle n\log n}n\log n     |        n     |        No     |        Insertion & Selection     |        Finds all the longest increasing subsequences in O(n log n).     |        
| Smoothsort     |        n     |        {\displaystyle n\log n}n\log n     |        {\displaystyle n\log n}n\log n     |        1     |        No     |        Selection     |        An adaptive variant of heapsort based upon the Leonardo sequence rather than a traditional binary heap.     |        
| Strand sort     |        n     |        {\displaystyle n^{2}}n^{2}     |        {\displaystyle n^{2}}n^{2}     |        n     |        Yes     |        Selection     |             |        
| Tournament sort     |        {\displaystyle n\log n}n\log n     |        {\displaystyle n\log n}n\log n     |        {\displaystyle n\log n}n\log n     |        n[10]     |        No     |        Selection     |        Variation of Heap Sort.     |        
| Cocktail sort     |        n     |        {\displaystyle n^{2}}n^{2}     |        {\displaystyle n^{2}}n^{2}     |        1     |        Yes     |        Exchanging     |             |        
| Comb sort     |        {\displaystyle n\log n}n\log n     |        {\displaystyle n^{2}}n^{2}     |        {\displaystyle n^{2}}n^{2}     |        1     |        No     |        Exchanging     |        Faster than bubble sort on average.     |        
| Gnome sort     |        n     |        {\displaystyle n^{2}}n^{2}     |        {\displaystyle n^{2}}n^{2}     |        1     |        Yes     |        Exchanging     |        Tiny code size.     |        
| UnShuffle Sort[11]     |        n     |        kn     |        kn     |        In-place for linked lists. n * sizeof(link) for array. n+1 for array?     |        No     |        Distribution and Merge     |        No exchanges are performed. The parameter k is proportional to the entropy in the input. k = 1 for ordered or reverse ordered input.     |        
| Franceschini's method[12]     |        —     |        {\displaystyle n\log n}n\log n     |        {\displaystyle n\log n}n\log n     |        1     |        Yes     |        ?     |             |        
| Block sort     |        n     |        {\displaystyle n\log n}n\log n     |        {\displaystyle n\log n}n\log n     |        1     |        Yes     |        Insertion & Merging     |        Combine a block-based {\displaystyle O(n)}O(n) in-place merge algorithm[13] with a bottom-up merge sort.     |        
| Odd–even sort     |        n     |        {\displaystyle n^{2}}n^{2}     |        {\displaystyle n^{2}}n^{2}     |        1     |        Yes     |        Exchanging     |        Can be run on parallel processors easily.     |        
| Curve sort     |        n     |        {\displaystyle n^{2}}n^{2}     |        {\displaystyle n^{2}}n^{2}     |        {\displaystyle \Omega (1)\cap {\mathcal {O}}(n)}{\displaystyle \Omega (1)\cap {\mathcal {O}}(n)}     |        Yes     |        Insertion & counting     |        Adapts to the smoothness of data.     |        


Non-comparison sorts
The following table describes integer sorting algorithms and other sorting algorithms that are not comparison sorts. As such, they are not limited to Ω(n log n)[citation needed]. Complexities below assume n items to be sorted, with keys of size k, digit size d, and r the range of numbers to be sorted. Many of them are based on the assumption that the key size is large enough that all entries have unique key values, and hence that n ≪ 2k, where ≪ means "much less than". In the unit-cost random access machine model, algorithms with running time of {\displaystyle \scriptstyle n\cdot {\frac {k}{d}}}{\displaystyle \scriptstyle n\cdot {\frac {k}{d}}}, such as radix sort, still take time proportional to Θ(n log n), because n is limited to be not more than {\displaystyle 2^{\frac {k}{d}}}2^{\frac {k}{d}}, and a larger number of elements to sort would require a bigger k in order to store them in the memory.[14]

Non-comparison sorts
Name     |        Best     |        Average     |        Worst     |        Memory     |        Stable     |        n ≪ 2k     |        Notes
Pigeonhole sort     |        —     |        {\displaystyle n+2^{k}}n+2^{k}     |        {\displaystyle n+2^{k}}n+2^{k}     |        {\displaystyle 2^{k}}2^{k}     |        Yes     |        Yes     |        
Bucket sort (uniform keys)     |        —     |        {\displaystyle n+k}n+k     |        {\displaystyle n^{2}\cdot k}n^{2}\cdot k     |        {\displaystyle n\cdot k}n\cdot k     |        Yes     |        No     |        Assumes uniform distribution of elements from the domain in the array.[15]
Bucket sort (integer keys)     |        —     |        {\displaystyle n+r}n+r     |        {\displaystyle n+r}n+r     |        {\displaystyle n+r}n+r     |        Yes     |        Yes     |        If r is {\displaystyle O(n)}O(n), then average time complexity is {\displaystyle O(n)}O(n).[16]
Counting sort     |        —     |        {\displaystyle n+r}n+r     |        {\displaystyle n+r}n+r     |        {\displaystyle n+r}n+r     |        Yes     |        Yes     |        If r is {\displaystyle O(n)}O(n), then average time complexity is {\displaystyle O(n)}O(n).[15]
LSD Radix Sort     |        —     |        {\displaystyle n\cdot {\frac {k}{d}}}n\cdot {\frac {k}{d}}     |        {\displaystyle n\cdot {\frac {k}{d}}}n\cdot {\frac {k}{d}}     |        {\displaystyle n+2^{d}}n+2^{d}     |        Yes     |        No     |        ,[15][16] {\displaystyle {\frac {k}{d}}}{\frac {k}{d}} recursion levels, 2d for count array.
MSD Radix Sort     |        —     |        {\displaystyle n\cdot {\frac {k}{d}}}n\cdot {\frac {k}{d}}     |        {\displaystyle n\cdot {\frac {k}{d}}}n\cdot {\frac {k}{d}}     |        {\displaystyle n+2^{d}}n+2^{d}     |        Yes     |        No     |        Stable version uses an external array of size n to hold all of the bins.
MSD Radix Sort (in-place)     |        —     |        {\displaystyle n\cdot {\frac {k}{d}}}n\cdot {\frac {k}{d}}     |        {\displaystyle n\cdot {\frac {k}{d}}}n\cdot {\frac {k}{d}}     |        {\displaystyle 2^{d}}2^{d}     |        No     |        No     |        d=1 for in-place, {\displaystyle k/1}{\displaystyle k/1} recursion levels, no count array.
Spreadsort     |        n     |        {\displaystyle n\cdot {\frac {k}{d}}}n\cdot {\frac {k}{d}}     |        {\displaystyle n\cdot \left({{\frac {k}{s}}+d}\right)}n\cdot \left({{\frac {k}{s}}+d}\right)     |        {\displaystyle {\frac {k}{d}}\cdot 2^{d}}{\frac {k}{d}}\cdot 2^{d}     |        No     |        No     |        Asymptotic are based on the assumption that n ≪ 2k, but the algorithm does not require this.
Burstsort     |        —     |        {\displaystyle n\cdot {\frac {k}{d}}}n\cdot {\frac {k}{d}}     |        {\displaystyle n\cdot {\frac {k}{d}}}n\cdot {\frac {k}{d}}     |        {\displaystyle n\cdot {\frac {k}{d}}}n\cdot {\frac {k}{d}}     |        No     |        No     |        Has better constant factor than radix sort for sorting strings. Though relies somewhat on specifics of commonly encountered strings.
Flashsort     |        n     |        {\displaystyle n+r}n+r     |        {\displaystyle n^{2}}n^{2}     |        n     |        No     |        No     |        Requires uniform distribution of elements from the domain in the array to run in linear time. If distribution is extremely skewed then it can go quadratic if underlying sort is quadratic (it is usually an insertion sort). In-place version is not stable.
Postman sort     |        —     |        {\displaystyle n\cdot {\frac {k}{d}}}n\cdot {\frac {k}{d}}     |        {\displaystyle n\cdot {\frac {k}{d}}}n\cdot {\frac {k}{d}}     |        {\displaystyle n+2^{d}}n+2^{d}     |        —     |        No     |        A variation of bucket sort, which works very similar to MSD Radix Sort. Specific to post service needs.
Samplesort can be used to parallelize any of the non-comparison sorts, by efficiently distributing data into several buckets and then passing down sorting to several processors, with no need to merge as buckets are already sorted between each other.

Others
Some algorithms are slow compared to those discussed above, such as the bogosort with unbounded run time and the stooge sort which has O(n2.7) run time. These sorts are usually described for educational purposes in order to demonstrate how run time of algorithms is estimated. The following table describes some sorting algorithms that are impractical for real-life use in traditional software contexts due to extremely poor performance or specialized hardware requirements.

Name     |        Best     |        Average     |        Worst     |        Memory     |        Stable     |        Comparison     |        Other notes
Bead sort     |        n     |        S     |        S     |        {\displaystyle n^{2}}n^{2}     |        N/A     |        No     |        Works only with positive integers. Requires specialized hardware for it to run in guaranteed {\displaystyle O(n)}O(n) time. There is a possibility for software implementation, but running time will be {\displaystyle O(S)}{\displaystyle O(S)}, where S is sum of all integers to be sorted, in case of small integers it can be considered to be linear.
Simple pancake sort     |        —     |        n     |        n     |        {\displaystyle \log n}\log n     |        No     |        Yes     |        Count is number of flips.
Spaghetti (Poll) sort     |        n     |        n     |        n     |        {\displaystyle n^{2}}n^{2}     |        Yes     |        Polling     |        This is a linear-time, analog algorithm for sorting a sequence of items, requiring O(n) stack space, and the sort is stable. This requires n parallel processors. See spaghetti sort#Analysis.
Sorting network     |        {\displaystyle \log ^{2}n}\log ^{2}n     |        {\displaystyle \log ^{2}n}\log ^{2}n     |        {\displaystyle \log ^{2}n}\log ^{2}n     |        {\displaystyle n\log ^{2}n}n\log ^{2}n     |        Varies (stable sorting networks require more comparisons)     |        Yes     |        Order of comparisons are set in advance based on a fixed network size. Impractical for more than 32 items.[disputed – discuss]
Bitonic sorter     |        {\displaystyle \log ^{2}n}\log ^{2}n     |        {\displaystyle \log ^{2}n}\log ^{2}n     |        {\displaystyle \log ^{2}n}\log ^{2}n     |        {\displaystyle n\log ^{2}n}n\log ^{2}n     |        No     |        Yes     |        An effective variation of Sorting networks.
Bogosort     |        n     |        {\displaystyle (n\times n!)}{\displaystyle (n\times n!)}     |        {\displaystyle \infty }\infty      |        1     |        No     |        Yes     |        Random shuffling. Used for example purposes only, as sorting with unbounded worst case running time.
Stooge sort     |        {\displaystyle n^{\log 3/\log 1.5}}n^{{\log 3/\log 1.5}}     |        {\displaystyle n^{\log 3/\log 1.5}}n^{{\log 3/\log 1.5}}     |        {\displaystyle n^{\log 3/\log 1.5}}n^{{\log 3/\log 1.5}}     |        n     |        No     |        Yes     |        Slower than most of the sorting algorithms (even naive ones) with a time complexity of O(nlog 3 / log 1.5 ) = O(n2.7095...).
Theoretical computer scientists have detailed other sorting algorithms that provide better than O(n log n) time complexity assuming additional constraints, including:

Han's algorithm, a deterministic algorithm for sorting keys from a domain of finite size, taking O(n log log n) time and O(n) space.[17]
Thorup's algorithm, a randomized algorithm for sorting keys from a domain of finite size, taking O(n log log n) time and O(n) space.[18]
A randomized integer sorting algorithm taking {\displaystyle O\left(n{\sqrt {\log \log n}}\right)}{\displaystyle O\left(n{\sqrt {\log \log n}}\right)} expected time and O(n) space.[19]