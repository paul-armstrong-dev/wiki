{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"All Personal Notes","title":"Home"},{"location":"#all-personal-notes","text":"","title":"All Personal Notes"},{"location":"about/","text":"Subiectis Eurotas et reperitur in despice scylla Vinctae aequora nate fidae esse terram respiramen Lorem markdownum fatigant tulit enim postera , cadit, si nostra tecta mirabantur culpa tenues . Et madidis proelia possim caput teneris, et erigitur herbas mentem aequales sonant praetemptanda umbrarum. Alia vada Denique, dextrum bis: ea quod reseratis quod domus! Qui moles cupidine Putes procul, tamen bellum tangentiaque plus novus Pyracmon in casas, dextra grave cognovit ante. Agit illa sagitta victima inter et vehebat experiar concutio doctior venenata aquarum quippe micabant in peregit quoque. Quem non contigit; feriente maximus, quas at summo? if (operating) { bookmark_cluster_cache.megabit_data(46, commercial); ddr_sampling = readGigaflops; default_supply_token = fat; } else { page(barTweet, pitch); radcab_terminal_server(2, hdd, softDragBespoke); screenshotMamp -= seoMultithreadingXp - lifoSolid; } var artificial = 5; tebibyte.linkedin(dockIp(thermistorBar.rom_malware(icmp, p_file), 4)); Nec natam morte Cutis discedere anni patuit ad poposcerat ultima; nec vetuit Tyria, iam vestra deus magna frondes murmure veteris diduxit? Minos tenebat terret se et Triptolemo nefandam damnum. Inpensior fuit suo clavigeram coepta; flectitur cornaque mittunt quae, non tulit est quin cupidusque. Esto Ortygie egi nebulas vertitur esse redire nec cursus patri Iunoni; zephyri te crede anhelis. Tonanti montibus inter ad magnae Procrin intrata taurorum tendentes liber, alia patre Pirithoi dissimulat ante. Dum qui fundebat, ut edocuit tacito, si tenuere vultu, ultimus saxa! O domum, quoque igitur tetigere lebetes diro verba clamor. Teste et summo cladis indignantem tandem enixa utroque feliciter bifurcum virum formae: et cum virgineos! Nefanda nec velox nescius Iove esset Mari dixit evellere referam nivosos prius quae, dari custos dies temporis, et famulus operum: huc praesens. Sororum redditus victum voluptas ostendere rediit cretosaque fortuna Phoebus palato ab dederant gurgite tela pugnae , fundamine cacumine dicta? Qui natae vetitus. Aut fallit petit ferenti, venitque de maturus omnibus. Iocosa cultum toto exierat dum quam regna se viribus urnam et. Per totus Philippi Profecto fuit Ab amor enses et refert pro forent Detur per margine manus denique exstabat transieram Perdere dubitat Phoebus quae Venitque patuere cuncta Et protinus nescio praestantes tandem inconcessisque transtra tinguet; licet fictum conpressit umeris. Trepidoque tempora haud ait mora vocatos usque . Diu tu colla roga tantum satis Cypron suasit anilia duritia Ilios saepe, age sonat. Solibus miserarum At ferrum freta non: hastam ferre delatus ramis umbrae .","title":"Subiectis Eurotas et reperitur in despice scylla"},{"location":"about/#subiectis-eurotas-et-reperitur-in-despice-scylla","text":"","title":"Subiectis Eurotas et reperitur in despice scylla"},{"location":"about/#vinctae-aequora-nate-fidae-esse-terram-respiramen","text":"Lorem markdownum fatigant tulit enim postera , cadit, si nostra tecta mirabantur culpa tenues . Et madidis proelia possim caput teneris, et erigitur herbas mentem aequales sonant praetemptanda umbrarum. Alia vada Denique, dextrum bis: ea quod reseratis quod domus!","title":"Vinctae aequora nate fidae esse terram respiramen"},{"location":"about/#qui-moles-cupidine","text":"Putes procul, tamen bellum tangentiaque plus novus Pyracmon in casas, dextra grave cognovit ante. Agit illa sagitta victima inter et vehebat experiar concutio doctior venenata aquarum quippe micabant in peregit quoque. Quem non contigit; feriente maximus, quas at summo? if (operating) { bookmark_cluster_cache.megabit_data(46, commercial); ddr_sampling = readGigaflops; default_supply_token = fat; } else { page(barTweet, pitch); radcab_terminal_server(2, hdd, softDragBespoke); screenshotMamp -= seoMultithreadingXp - lifoSolid; } var artificial = 5; tebibyte.linkedin(dockIp(thermistorBar.rom_malware(icmp, p_file), 4));","title":"Qui moles cupidine"},{"location":"about/#nec-natam-morte","text":"Cutis discedere anni patuit ad poposcerat ultima; nec vetuit Tyria, iam vestra deus magna frondes murmure veteris diduxit? Minos tenebat terret se et Triptolemo nefandam damnum. Inpensior fuit suo clavigeram coepta; flectitur cornaque mittunt quae, non tulit est quin cupidusque. Esto Ortygie egi nebulas vertitur esse redire nec cursus patri Iunoni; zephyri te crede anhelis. Tonanti montibus inter ad magnae Procrin intrata taurorum tendentes liber, alia patre Pirithoi dissimulat ante. Dum qui fundebat, ut edocuit tacito, si tenuere vultu, ultimus saxa! O domum, quoque igitur tetigere lebetes diro verba clamor. Teste et summo cladis indignantem tandem enixa utroque feliciter bifurcum virum formae: et cum virgineos!","title":"Nec natam morte"},{"location":"about/#nefanda-nec-velox-nescius-iove-esset","text":"Mari dixit evellere referam nivosos prius quae, dari custos dies temporis, et famulus operum: huc praesens. Sororum redditus victum voluptas ostendere rediit cretosaque fortuna Phoebus palato ab dederant gurgite tela pugnae , fundamine cacumine dicta? Qui natae vetitus. Aut fallit petit ferenti, venitque de maturus omnibus. Iocosa cultum toto exierat dum quam regna se viribus urnam et. Per totus Philippi Profecto fuit Ab amor enses et refert pro forent Detur per margine manus denique exstabat transieram Perdere dubitat Phoebus quae Venitque patuere cuncta Et protinus nescio praestantes tandem inconcessisque transtra tinguet; licet fictum conpressit umeris. Trepidoque tempora haud ait mora vocatos usque . Diu tu colla roga tantum satis Cypron suasit anilia duritia Ilios saepe, age sonat. Solibus miserarum At ferrum freta non: hastam ferre delatus ramis umbrae .","title":"Nefanda nec velox nescius Iove esset"},{"location":"rtlnl-standards/","text":"rtl-py-coding-standards Centralized location for all RTL-python coding standards. Values \"Build tools for others that you want to be built for you.\" - Kenneth Reitz \"Simplicity is alway better than functionality.\" - Pieter Hintjens \"Fit the 90% use-case. Ignore the nay sayers.\" - Kenneth Reitz \"Beautiful is better than ugly.\" - [PEP 20] Use the case that handles every case. (summary of google docs) Single Object Responsibility General Development Guidelines \"Explicit is better than implicit\" - [PEP 20] \"Readability counts.\" - [PEP 20] \"Anybody can fix anything.\" - [Khan Academy Development Docs] Fix each broken window (bad design, wrong decision, or poor code) as soon as it is discovered . \"Now is better than never.\" - [PEP 20] Test ruthlessly. Write docs for new features. Even more important that Test-Driven Development-- Human-Driven Development These guidelines may--and probably will--change. In Particular Style Follow [PEP 8], when sensible. Use f-strings for format if on python3.6> from - pep498 Max line length of 100 Double quotes for ALL strings (handles every case) from - GoogleStyle Follow single object responsibility as strictly as possible, Encapsulate operations into well named functions. Follow sonarcube max logical complexity limit per function (no functions with more than 15 operations) Naming Variables, functions, methods, packages, modules lower_case_with_underscores Classes and Exceptions CapWords Protected methods and internal functions _single_leading_underscore(self, ...) Private methods __double_leading_underscore(self, ...) Constants ALL_CAPS_WITH_UNDERSCORES General Naming Guidelines Avoid one-letter variables (esp. l , O , I ). Ideally name functions as verbosely as possible According to their function ie: def main(self): \"\"\" Main function for the daily process :return: \"\"\" df = self.prepare_data(self.open_file()) self.save_file(df=df, location=\"local\") self.delete_from_sql() self.write_to_sql(dataframe=df) self.archive_file() self.create_view() exit(0) Exceptions : In very short blocks, when the meaning is clearly visible from the immediate context Ideally do not use excepts where possible, try actually handle the error through testing and correcting code. Never except Exception Avoid redundant labeling. Yes import audio core = audio.Core() controller = audio.Controller() No import audio core = audio.AudioCore() controller = audio.AudioController() Prefer \"reverse notation\". Yes elements = ... elements_active = ... elements_defunct = ... No elements = ... active_elements = ... defunct_elements ... Avoid getter and setter methods. Yes person.age = 42 No person.set_age(42) Indentation Use 4 spaces--never tabs. Enough said. Line Length 80 seems too strict so can compromise at 100 Ideally configure IDE to warn on this Imports Import entire modules instead of individual symbols within a module. For example, for a top-level module canteen that has a file canteen/sessions.py , Yes import canteen import canteen.sessions from canteen import sessions No from canteen import get_user # Symbol from canteen/__init__.py from canteen.sessions import get_session # Symbol from canteen/sessions.py Exception : For third-party code where documentation explicitly says to import individual symbols. Rationale : Avoids circular imports. See here . Put all imports at the top of the page with three sections, each separated by a blank line, in this order: System imports Third-party imports Local source tree imports Rationale : Makes it clear where each module is coming from. https://www.python.org/dev/peps/pep-0008/#imports -- Rtl Specfics Environment variables Would like to (likely more feasible once we have vault) Standardise environment variables(along with the names) (eg: sql_user = os.environ['MSSQL_USER'] sql_password = os.environ['MSSQL_PASSWORD'] sql_server = os.environ['MSSQL_SERVER_IP'] sql_port = os.environ['MSSQL_PORT'] sql_db = os.environ['MSSQL_DB'] -- Similary, if possible, would like to standardize date variables and how they are handled, - Likely the easiest option for receiving functions is to have start_date and end_date any time we use a date. So run(start_date=\"\", end_date=\"\") - But would also like to see with regards to days back etc, I know this is used a lot on SQL especially and would like them to be handled in the same Way, so days_back = start_date end_date, but not startdate = days_back(-day) https://dateutil.readthedocs.io/en/stable/relativedelta.html logging https://github.com/Delgan/loguru templating https://github.com/cookiecutter/cookiecutter Provide some simple examples on the above Class template pycharm settings repo? To be discusses requirements Needs to be in the repo, and versioned . Steps to create correctly, everytime. create venv for repo as start of work Do pip > requirements.txt before checking in repo Checkin requirements.txt -- Notes on performance: List Comprehension List comprehensions should only be used when actually storing the output of the list comprehension, If you are modifying a list or dict rather use the traditional syntax: for x in y: x=x+1 Numpy There\u2019s a couple of points we can follow when looking to speed things up: - If there\u2019s a for-loop over an array, there\u2019s a good chance we can replace it with some built-in Numpy function - If we see any type of math, there\u2019s a good chance we can replace it with some built-in Numpy function https://towardsdatascience.com/one-simple-trick-for-speeding-up-your-python-code-with-numpy-1afc846db418 https://www.datacamp.com/courses/writing-efficient-python-code -- . env files Always close values in \"\", can handle every variable type -- Repo naming conventions: gateway_lambda_vod-exitlogs/ Useful docs & further reading: https://github.com/joshnh/Git-Commands https://12factor.net/ Sources https://www.python.org/dev/peps/pep-0008/#imports https://www.python.org/dev/peps/pep-0498/ https://google.github.io/styleguide/pyguide.html http://www.moderndescartes.com/essays/python_list_comprehensions/index.html https://medium.com/@collectiveacuity/argparse-vs-click-227f53f023dc https://github.com/reddit-archive/reddit/wiki/PythonImportGuidelines https://code.google.com/archive/p/soc/wikis/PythonStyleGuide.wiki https://docs.openstack.org/hacking/latest/ https://www.python.org/dev/peps/pep-0008/","title":"rtl-py-coding-standards"},{"location":"rtlnl-standards/#rtl-py-coding-standards","text":"Centralized location for all RTL-python coding standards.","title":"rtl-py-coding-standards"},{"location":"rtlnl-standards/#values","text":"\"Build tools for others that you want to be built for you.\" - Kenneth Reitz \"Simplicity is alway better than functionality.\" - Pieter Hintjens \"Fit the 90% use-case. Ignore the nay sayers.\" - Kenneth Reitz \"Beautiful is better than ugly.\" - [PEP 20] Use the case that handles every case. (summary of google docs) Single Object Responsibility","title":"Values"},{"location":"rtlnl-standards/#general-development-guidelines","text":"\"Explicit is better than implicit\" - [PEP 20] \"Readability counts.\" - [PEP 20] \"Anybody can fix anything.\" - [Khan Academy Development Docs] Fix each broken window (bad design, wrong decision, or poor code) as soon as it is discovered . \"Now is better than never.\" - [PEP 20] Test ruthlessly. Write docs for new features. Even more important that Test-Driven Development-- Human-Driven Development These guidelines may--and probably will--change.","title":"General Development Guidelines"},{"location":"rtlnl-standards/#in-particular","text":"","title":"In Particular"},{"location":"rtlnl-standards/#style","text":"Follow [PEP 8], when sensible. Use f-strings for format if on python3.6> from - pep498 Max line length of 100 Double quotes for ALL strings (handles every case) from - GoogleStyle Follow single object responsibility as strictly as possible, Encapsulate operations into well named functions. Follow sonarcube max logical complexity limit per function (no functions with more than 15 operations)","title":"Style"},{"location":"rtlnl-standards/#naming","text":"Variables, functions, methods, packages, modules lower_case_with_underscores Classes and Exceptions CapWords Protected methods and internal functions _single_leading_underscore(self, ...) Private methods __double_leading_underscore(self, ...) Constants ALL_CAPS_WITH_UNDERSCORES","title":"Naming"},{"location":"rtlnl-standards/#general-naming-guidelines","text":"Avoid one-letter variables (esp. l , O , I ). Ideally name functions as verbosely as possible According to their function ie: def main(self): \"\"\" Main function for the daily process :return: \"\"\" df = self.prepare_data(self.open_file()) self.save_file(df=df, location=\"local\") self.delete_from_sql() self.write_to_sql(dataframe=df) self.archive_file() self.create_view() exit(0) Exceptions : In very short blocks, when the meaning is clearly visible from the immediate context Ideally do not use excepts where possible, try actually handle the error through testing and correcting code. Never except Exception Avoid redundant labeling. Yes import audio core = audio.Core() controller = audio.Controller() No import audio core = audio.AudioCore() controller = audio.AudioController() Prefer \"reverse notation\". Yes elements = ... elements_active = ... elements_defunct = ... No elements = ... active_elements = ... defunct_elements ... Avoid getter and setter methods. Yes person.age = 42 No person.set_age(42)","title":"General Naming Guidelines"},{"location":"rtlnl-standards/#indentation","text":"Use 4 spaces--never tabs. Enough said.","title":"Indentation"},{"location":"rtlnl-standards/#line-length","text":"80 seems too strict so can compromise at 100 Ideally configure IDE to warn on this","title":"Line Length"},{"location":"rtlnl-standards/#imports","text":"Import entire modules instead of individual symbols within a module. For example, for a top-level module canteen that has a file canteen/sessions.py , Yes import canteen import canteen.sessions from canteen import sessions No from canteen import get_user # Symbol from canteen/__init__.py from canteen.sessions import get_session # Symbol from canteen/sessions.py Exception : For third-party code where documentation explicitly says to import individual symbols. Rationale : Avoids circular imports. See here . Put all imports at the top of the page with three sections, each separated by a blank line, in this order: System imports Third-party imports Local source tree imports Rationale : Makes it clear where each module is coming from. https://www.python.org/dev/peps/pep-0008/#imports --","title":"Imports"},{"location":"rtlnl-standards/#rtl-specfics","text":"","title":"Rtl Specfics"},{"location":"rtlnl-standards/#environment-variables","text":"Would like to (likely more feasible once we have vault) Standardise environment variables(along with the names) (eg: sql_user = os.environ['MSSQL_USER'] sql_password = os.environ['MSSQL_PASSWORD'] sql_server = os.environ['MSSQL_SERVER_IP'] sql_port = os.environ['MSSQL_PORT'] sql_db = os.environ['MSSQL_DB'] -- Similary, if possible, would like to standardize date variables and how they are handled, - Likely the easiest option for receiving functions is to have start_date and end_date any time we use a date. So run(start_date=\"\", end_date=\"\") - But would also like to see with regards to days back etc, I know this is used a lot on SQL especially and would like them to be handled in the same Way, so days_back = start_date end_date, but not startdate = days_back(-day) https://dateutil.readthedocs.io/en/stable/relativedelta.html","title":"Environment variables"},{"location":"rtlnl-standards/#logging","text":"https://github.com/Delgan/loguru","title":"logging"},{"location":"rtlnl-standards/#templating","text":"https://github.com/cookiecutter/cookiecutter Provide some simple examples on the above Class template","title":"templating"},{"location":"rtlnl-standards/#pycharm-settings-repo","text":"To be discusses","title":"pycharm settings repo?"},{"location":"rtlnl-standards/#requirements","text":"Needs to be in the repo, and versioned . Steps to create correctly, everytime. create venv for repo as start of work Do pip > requirements.txt before checking in repo Checkin requirements.txt --","title":"requirements"},{"location":"rtlnl-standards/#notes-on-performance","text":"","title":"Notes on performance:"},{"location":"rtlnl-standards/#list-comprehension","text":"List comprehensions should only be used when actually storing the output of the list comprehension, If you are modifying a list or dict rather use the traditional syntax: for x in y: x=x+1","title":"List Comprehension"},{"location":"rtlnl-standards/#numpy","text":"There\u2019s a couple of points we can follow when looking to speed things up: - If there\u2019s a for-loop over an array, there\u2019s a good chance we can replace it with some built-in Numpy function - If we see any type of math, there\u2019s a good chance we can replace it with some built-in Numpy function https://towardsdatascience.com/one-simple-trick-for-speeding-up-your-python-code-with-numpy-1afc846db418 https://www.datacamp.com/courses/writing-efficient-python-code --","title":"Numpy"},{"location":"rtlnl-standards/#env-files","text":"Always close values in \"\", can handle every variable type --","title":". env files"},{"location":"rtlnl-standards/#repo-naming-conventions","text":"gateway_lambda_vod-exitlogs/","title":"Repo naming conventions:"},{"location":"rtlnl-standards/#useful-docs-further-reading","text":"https://github.com/joshnh/Git-Commands https://12factor.net/","title":"Useful docs &amp; further reading:"},{"location":"rtlnl-standards/#sources","text":"https://www.python.org/dev/peps/pep-0008/#imports https://www.python.org/dev/peps/pep-0498/ https://google.github.io/styleguide/pyguide.html http://www.moderndescartes.com/essays/python_list_comprehensions/index.html https://medium.com/@collectiveacuity/argparse-vs-click-227f53f023dc https://github.com/reddit-archive/reddit/wiki/PythonImportGuidelines https://code.google.com/archive/p/soc/wikis/PythonStyleGuide.wiki https://docs.openstack.org/hacking/latest/ https://www.python.org/dev/peps/pep-0008/","title":"Sources"},{"location":"rtlnl-testing/","text":"Testing in Python There are several ways of doing testing in Python and choosing the right one it's a tricky job. There are some assumptions that we need make in order to be able to standardize the way we can create and run tests both locally and in the CI/CD. We can split this subject in subsections: Unit Tests Integration Tests Unit Tests The most basic type of test. This is what the code-coverage in general they count. Becuase the unit test will evaluate a particular function , the code coverage is able to identify which part of the code in the function is being executed. Testing single functions is not as easy as it seems. It all depends how the code has been designed and without proper usage of the SOLID principles, we are not able to create tests. Let's assume that the code is actually testable. The Python Standard Library comes with a unittest package that helps the developer in writing tests. unittest contains both a testing framework and a test runner . unittest has some important requirements for writing and executing tests. More information about this package can be found in the official documentation . unittest requires that: You put your tests into classes as methods You use a series of special assertion methods in the unittest.TestCase class instead of the built-in assert statement In your code you need to: Import unittest from the standard library Create a class called TestSomething that inherits from the TestCase class Convert the test functions into methods by adding self as the first argument Use the entry point to call unittest.main() Let's take a look at this example: import unittest class TestSum(unittest.TestCase): def test_sum(self): self.assertEqual(sum([1, 2, 3]), 6, \"Should be 6\") def test_sum_tuple(self): self.assertEqual(sum((1, 2, 2)), 6, \"Should be 6\") if __name__ == '__main__': unittest.main() Assuming that the above code is in a file called test_sum.py , we can run the tests with simply cally python test_sum.py . Test files can be put in a separate folder called tests at root level. Because your code should be able to be called from anywhere since it's in a package, having all the tests in one place makes it easy to work on them. Since we also have different type of tests, we shoudl put the unit tests in a folder called unit . See the tree below as example: project/ \u2502 \u251c\u2500\u2500 my_app/ \u2502 \u2514\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 tests/ | \u251c\u2500\u2500 unit/ | \u251c\u2500\u2500 __init__.py | \u2514\u2500\u2500 test_sum.py | \u2514\u2500\u2500 integration/ \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 test_integration.py Test Discovery This is an important step especially when you have multiple test files. We don't want to run tests one by one, but we want to run all of them in one single command. Test Discovery is a functionality that will automatically run all the tests that is in the code base. In order to make it possible, we need to respect the following rule: All of the test files must be modules or packages (including namespace packages) importable from the top-level directory of the project (this means that their filenames must be valid identifiers). To run the test discovery, you can execute the following command from the root of your project: python -m unittest discover . SetUp and TearDown There are two special functions that can be used in order to initialize some objects that are needed for your tests. setUp() is a special method that is called before the tests in the class are run. This is useful, for example, if you need to upload some data into a testing database in order to check the functionality. Becuase you can initialize things, it's important that you clean them up to avoid inconsistency in your tests. tearDown() is a special function that is run when all the tests are being executed independently whether they failes or passed. If you raise an exception in your tests, then the function tearDown() won't be called becuase the program exited before completing. Make sure that you are able to handle all the errors in your test and make them fail with appropriate instructions rather then brutally terminate with an exception. More information can be found at this link . Integration Tests Integration testing is the testing of multiple components of the application to check that they work together. Integration testing might require acting like a consumer or user of the application by: Calling an HTTP REST API Calling a Python API Calling a web service Running a command line The most significant difference is that integration tests are checking more components at once and therefore will have more side effects than a unit test. Also, integration tests will require more fixtures to be in place, like a database, a network socket, or a configuration file. This is why it\u2019s good practice to separate your unit tests and your integration tests. The creation of fixtures required for an integration like a test database and the test cases themselves often take a lot longer to execute than unit tests, so you may only want to run integration tests before you push to production instead of once on every commit. This can be set it up in the CI/CD because we make the distincion on a folder level. Since we are using Docker and docker-compose it's easy to reproduce the \"production\" environment locally. Hence, by having docker-compose up and running you can write intergation tests exaclty like the unit tests explained above. The difference is that you may call the \"upper most\" function and wait for the result to verify that all the components are working together. That's the goal of Integration Tests. There are no particular libraries to use. The Standard Library is sufficient to cover all the type of tests. Concourse CI Currently we are investigating how to run multiple components in our CI system so that we can run integration tests when merging to master and before going to production.","title":"Testing in Python"},{"location":"rtlnl-testing/#testing-in-python","text":"There are several ways of doing testing in Python and choosing the right one it's a tricky job. There are some assumptions that we need make in order to be able to standardize the way we can create and run tests both locally and in the CI/CD. We can split this subject in subsections: Unit Tests Integration Tests","title":"Testing in Python"},{"location":"rtlnl-testing/#unit-tests","text":"The most basic type of test. This is what the code-coverage in general they count. Becuase the unit test will evaluate a particular function , the code coverage is able to identify which part of the code in the function is being executed. Testing single functions is not as easy as it seems. It all depends how the code has been designed and without proper usage of the SOLID principles, we are not able to create tests. Let's assume that the code is actually testable. The Python Standard Library comes with a unittest package that helps the developer in writing tests. unittest contains both a testing framework and a test runner . unittest has some important requirements for writing and executing tests. More information about this package can be found in the official documentation . unittest requires that: You put your tests into classes as methods You use a series of special assertion methods in the unittest.TestCase class instead of the built-in assert statement In your code you need to: Import unittest from the standard library Create a class called TestSomething that inherits from the TestCase class Convert the test functions into methods by adding self as the first argument Use the entry point to call unittest.main() Let's take a look at this example: import unittest class TestSum(unittest.TestCase): def test_sum(self): self.assertEqual(sum([1, 2, 3]), 6, \"Should be 6\") def test_sum_tuple(self): self.assertEqual(sum((1, 2, 2)), 6, \"Should be 6\") if __name__ == '__main__': unittest.main() Assuming that the above code is in a file called test_sum.py , we can run the tests with simply cally python test_sum.py . Test files can be put in a separate folder called tests at root level. Because your code should be able to be called from anywhere since it's in a package, having all the tests in one place makes it easy to work on them. Since we also have different type of tests, we shoudl put the unit tests in a folder called unit . See the tree below as example: project/ \u2502 \u251c\u2500\u2500 my_app/ \u2502 \u2514\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 tests/ | \u251c\u2500\u2500 unit/ | \u251c\u2500\u2500 __init__.py | \u2514\u2500\u2500 test_sum.py | \u2514\u2500\u2500 integration/ \u251c\u2500\u2500 __init__.py \u2514\u2500\u2500 test_integration.py","title":"Unit Tests"},{"location":"rtlnl-testing/#test-discovery","text":"This is an important step especially when you have multiple test files. We don't want to run tests one by one, but we want to run all of them in one single command. Test Discovery is a functionality that will automatically run all the tests that is in the code base. In order to make it possible, we need to respect the following rule: All of the test files must be modules or packages (including namespace packages) importable from the top-level directory of the project (this means that their filenames must be valid identifiers). To run the test discovery, you can execute the following command from the root of your project: python -m unittest discover .","title":"Test Discovery"},{"location":"rtlnl-testing/#setup-and-teardown","text":"There are two special functions that can be used in order to initialize some objects that are needed for your tests. setUp() is a special method that is called before the tests in the class are run. This is useful, for example, if you need to upload some data into a testing database in order to check the functionality. Becuase you can initialize things, it's important that you clean them up to avoid inconsistency in your tests. tearDown() is a special function that is run when all the tests are being executed independently whether they failes or passed. If you raise an exception in your tests, then the function tearDown() won't be called becuase the program exited before completing. Make sure that you are able to handle all the errors in your test and make them fail with appropriate instructions rather then brutally terminate with an exception. More information can be found at this link .","title":"SetUp and TearDown"},{"location":"rtlnl-testing/#integration-tests","text":"Integration testing is the testing of multiple components of the application to check that they work together. Integration testing might require acting like a consumer or user of the application by: Calling an HTTP REST API Calling a Python API Calling a web service Running a command line The most significant difference is that integration tests are checking more components at once and therefore will have more side effects than a unit test. Also, integration tests will require more fixtures to be in place, like a database, a network socket, or a configuration file. This is why it\u2019s good practice to separate your unit tests and your integration tests. The creation of fixtures required for an integration like a test database and the test cases themselves often take a lot longer to execute than unit tests, so you may only want to run integration tests before you push to production instead of once on every commit. This can be set it up in the CI/CD because we make the distincion on a folder level. Since we are using Docker and docker-compose it's easy to reproduce the \"production\" environment locally. Hence, by having docker-compose up and running you can write intergation tests exaclty like the unit tests explained above. The difference is that you may call the \"upper most\" function and wait for the result to verify that all the components are working together. That's the goal of Integration Tests. There are no particular libraries to use. The Standard Library is sufficient to cover all the type of tests.","title":"Integration Tests"},{"location":"rtlnl-testing/#concourse-ci","text":"Currently we are investigating how to run multiple components in our CI system so that we can run integration tests when merging to master and before going to production.","title":"Concourse CI"},{"location":"big_data/comparisons/","text":"Hadoop vs Spark Hadoop Hadoop is an open-source framework that allows to store and process big data, in a distributed environment across clusters of computers. - Hadoop is designed to scale up from a single server to thousands of machines, where every machine is offering local computation and storage. - Hadoop is a registered trademark of the Apache software foundation. - It utilizes a simple programming model to perform the required operation among clusters. - All modules in Hadoop are designed with a fundamental assumption that hardware failures are common occurrences and should be dealt with by the framework. - It runs the application using the MapReduce algorithm, where data is processed in parallel on different CPU nodes. - In other words, the Hadoop framework is capable enough to develop applications, which are further capable of running on clusters of computers and they could perform a complete statistical analysis for a huge amount of data. - The core of Hadoop consists of: - Storage - Known as Hadoop Distributed File System - Processing - Known as the MapReduce programming model . - Hadoop basically split files into the large blocks and distribute them across the clusters, transfer package code into nodes to process data in parallel. - This approach dataset to be processed faster and more efficiently. - Other Hadoop modules are: - Hadoop common , which is a bunch of Java libraries and utilities returned by Hadoop modules. - These libraries provide a file system and operating system level abstraction, also contain required Java files and scripts to start Hadoop. - Hadoop Yarn is also a module, which is being used for job scheduling and cluster resource management. Spark Spark is an open-source cluster computing designed for fast computation. - It provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. - The main feature of Spark is in-memory cluster computing that increases the speed of an application. - Spark was built on the top of Hadoop MapReduce module and it extends the MapReduce model to efficiently use more type of computations which include Interactive Queries and Stream Processing. - Spark was introduced by the Apache software foundation, to speed up the Hadoop computational computing software process. - Spark has its own cluster management and is not a modified version of Hadoop. - Spark utilizes Hadoop in two ways: 1. storage 2. processing. - Since cluster management is arriving from Spark itself, it uses Hadoop for storage purposes only. - Spark is one of the Hadoop\u2019s subprojects which was developed in 2009, and later it became open source under a BSD license. - It has lots of wonderful features, by modifying certain modules and incorporating new modules. - It helps run an application in a Hadoop cluster, multiple times faster in memory. - This is made possible by reducing the number of read/write operations to disk. - It stores the intermediate processing data in memory, saving read/write operations. - Spark also provides built-in APIs in Java, Python or Scala. - Thus, one can write applications in multiple ways. - Spark not only provides a Map and Reduce strategy but also support SQL queries, Streaming data, Machine learning and Graph Algorithms. Comparison Key differences between Hadoop vs Spark Both Hadoop vs Spark are popular choices in the market; let us discuss some of the major Difference Between Hadoop and Spark: Hadoop is an open source framework which uses a MapReduce algorithm Spark is lightning fast cluster computing technology, which extends the MapReduce model to efficiently use with more type of computations. Hadoop\u2019s MapReduce model reads and writes from a disk, thus slow down the processing speed Spark reduces the number of read/write cycles to disk and store intermediate data in-memory, hence faster-processing speed. Hadoop requires developers to hand code each and every operation Spark is easy to program with RDD \u2013 Resilient Distributed Dataset. Hadoop MapReduce model provides a batch engine, hence dependent on different engines for other requirements Spark performs batch, interactive, Machine Learning and Streaming all in the same cluster. Hadoop is designed to handle batch processing efficiently Spark is designed to handle real-time data efficiently. Hadoop is a high latency computing framework, which does not have an interactive mode Spark is a low latency computing and can process data interactively. With Hadoop MapReduce, a developer can only process data in batch mode only Spark can process real-time data through Spark Streaming. Hadoop is designed to handle faults and failures, it is naturally resilient toward faults, hence a highly fault-tolerant system with Spark, RDD allows recovery of partitions on failed nodes. Hadoop needs an external job scheduler for example \u2013 Oozie to schedule complex flows Spark has in-memory computation, so it has its own flow scheduler. Hadoop is a cheaper option available while comparing it in terms of cost Spark requires a lot of RAM to run in-memory, thus increasing the cluster and hence cost. Hadoop Spark Category Basic Data processing engine Data analytics engine Usage Batch processing with a huge volume of data Process real-time data, from real-time events like Twitter, Facebook Latency High latency computing Low latency computing Data Process data in batch mode Can process interactively Ease of use Hadoop\u2019s MapReduce model is complex, need to handle low-level APIs Easier to use, abstraction enables a user to process data using high-level operators Scheduler External job scheduler is required In-memory computation, no external scheduler required Security Highly secure Less secure as compare to Hadoop Cost Less costly since MapReduce model provide a cheaper strategy Costlier than Hadoop since it has an in-memory solution Hadoop vs SQL Hadoop SQL It can be used for storing, processing, retrieving and pattern extraction from data across a wide range of formats. It can be used for storage, processing, retrieval and pattern mining of data stored in a relational database format only It works well for structured and unstructured data. It works only for structured data only It can many technology stacks on top of it each doing a specific task like HDFS, AVRO, Pig, HBase etc. SQL is a query language with specific syntax and a scheme to get around with things Data can be stored in the form of key-value pairs, tables, hash map etc. Data is stored in the form of tables only It supports NoSQL type data structures, columnar data structures etc. like MongoDB It works on the property of ACID It can be used to store and process log data, real-time data, images, videos, sensor data and other variety of data. Data variety is severely restricted in SQL Hadoop is used mainly in those applications where data volume is huge and systems like SQL cannot perform well. SQL can store a moderate volume of data INSERT, SELECT type statements are very fast in Hadoop compared to SQL SQL syntax are much slower when executed on millions of rows at a time Hadoop uses the concept of distributed computing, applies the principle of map-reduce and thus handle data available on multiple systems across multiple locations. SQL data sources are usually available on-premise or on a cloud. Thus it cannot exploit the advantages of distributed computing Hadoop based systems can be easily and cost-effectively scaled. Horizontal scaling is very cheap and as many computers can be connected to the network as desired thus it is scalable on demand. Buying an additional SQL server costs a fortune. If a system runs out of storage, additional racks and servers need to be purchased and configured which is expensive and time-consuming It is highly faulted tolerant. It has good fault tolerance It uses commodity hardware. It uses propriety hardware It is a free and open source. Most of the SQL systems are licensed Advanced machine learning and artificial intelligence techniques can be build using Hadoop. Support for ML and AI is highly limited on SQL and only a few companies provide that Using appropriate JDBC connectors, Hadoop can communicate with SQL systems and move data in between. SQL systems can also read and write data to Hadoop infrastructure Cloudera, Horton work, AWS are some of the providers of Hadoop systems. Microsoft, Oracle, SAP etc. are some of the well-known industry leaders in SQL systems Last but not the least, the learning curve of Hadoop for entry-level professionals, as well as a seasoned professional, is moderately hard. Starting with SQL systems is much easier for even entry-level professionals","title":"Comparisons"},{"location":"big_data/comparisons/#hadoop-vs-spark","text":"","title":"Hadoop vs Spark"},{"location":"big_data/comparisons/#hadoop","text":"Hadoop is an open-source framework that allows to store and process big data, in a distributed environment across clusters of computers. - Hadoop is designed to scale up from a single server to thousands of machines, where every machine is offering local computation and storage. - Hadoop is a registered trademark of the Apache software foundation. - It utilizes a simple programming model to perform the required operation among clusters. - All modules in Hadoop are designed with a fundamental assumption that hardware failures are common occurrences and should be dealt with by the framework. - It runs the application using the MapReduce algorithm, where data is processed in parallel on different CPU nodes. - In other words, the Hadoop framework is capable enough to develop applications, which are further capable of running on clusters of computers and they could perform a complete statistical analysis for a huge amount of data. - The core of Hadoop consists of: - Storage - Known as Hadoop Distributed File System - Processing - Known as the MapReduce programming model . - Hadoop basically split files into the large blocks and distribute them across the clusters, transfer package code into nodes to process data in parallel. - This approach dataset to be processed faster and more efficiently. - Other Hadoop modules are: - Hadoop common , which is a bunch of Java libraries and utilities returned by Hadoop modules. - These libraries provide a file system and operating system level abstraction, also contain required Java files and scripts to start Hadoop. - Hadoop Yarn is also a module, which is being used for job scheduling and cluster resource management.","title":"Hadoop"},{"location":"big_data/comparisons/#spark","text":"Spark is an open-source cluster computing designed for fast computation. - It provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. - The main feature of Spark is in-memory cluster computing that increases the speed of an application. - Spark was built on the top of Hadoop MapReduce module and it extends the MapReduce model to efficiently use more type of computations which include Interactive Queries and Stream Processing. - Spark was introduced by the Apache software foundation, to speed up the Hadoop computational computing software process. - Spark has its own cluster management and is not a modified version of Hadoop. - Spark utilizes Hadoop in two ways: 1. storage 2. processing. - Since cluster management is arriving from Spark itself, it uses Hadoop for storage purposes only. - Spark is one of the Hadoop\u2019s subprojects which was developed in 2009, and later it became open source under a BSD license. - It has lots of wonderful features, by modifying certain modules and incorporating new modules. - It helps run an application in a Hadoop cluster, multiple times faster in memory. - This is made possible by reducing the number of read/write operations to disk. - It stores the intermediate processing data in memory, saving read/write operations. - Spark also provides built-in APIs in Java, Python or Scala. - Thus, one can write applications in multiple ways. - Spark not only provides a Map and Reduce strategy but also support SQL queries, Streaming data, Machine learning and Graph Algorithms.","title":"Spark"},{"location":"big_data/comparisons/#comparison","text":"","title":"Comparison"},{"location":"big_data/comparisons/#key-differences-between-hadoop-vs-spark","text":"Both Hadoop vs Spark are popular choices in the market; let us discuss some of the major Difference Between Hadoop and Spark: Hadoop is an open source framework which uses a MapReduce algorithm Spark is lightning fast cluster computing technology, which extends the MapReduce model to efficiently use with more type of computations. Hadoop\u2019s MapReduce model reads and writes from a disk, thus slow down the processing speed Spark reduces the number of read/write cycles to disk and store intermediate data in-memory, hence faster-processing speed. Hadoop requires developers to hand code each and every operation Spark is easy to program with RDD \u2013 Resilient Distributed Dataset. Hadoop MapReduce model provides a batch engine, hence dependent on different engines for other requirements Spark performs batch, interactive, Machine Learning and Streaming all in the same cluster. Hadoop is designed to handle batch processing efficiently Spark is designed to handle real-time data efficiently. Hadoop is a high latency computing framework, which does not have an interactive mode Spark is a low latency computing and can process data interactively. With Hadoop MapReduce, a developer can only process data in batch mode only Spark can process real-time data through Spark Streaming. Hadoop is designed to handle faults and failures, it is naturally resilient toward faults, hence a highly fault-tolerant system with Spark, RDD allows recovery of partitions on failed nodes. Hadoop needs an external job scheduler for example \u2013 Oozie to schedule complex flows Spark has in-memory computation, so it has its own flow scheduler. Hadoop is a cheaper option available while comparing it in terms of cost Spark requires a lot of RAM to run in-memory, thus increasing the cluster and hence cost. Hadoop Spark Category Basic Data processing engine Data analytics engine Usage Batch processing with a huge volume of data Process real-time data, from real-time events like Twitter, Facebook Latency High latency computing Low latency computing Data Process data in batch mode Can process interactively Ease of use Hadoop\u2019s MapReduce model is complex, need to handle low-level APIs Easier to use, abstraction enables a user to process data using high-level operators Scheduler External job scheduler is required In-memory computation, no external scheduler required Security Highly secure Less secure as compare to Hadoop Cost Less costly since MapReduce model provide a cheaper strategy Costlier than Hadoop since it has an in-memory solution","title":"Key differences between Hadoop vs Spark"},{"location":"big_data/comparisons/#hadoop-vs-sql","text":"Hadoop SQL It can be used for storing, processing, retrieving and pattern extraction from data across a wide range of formats. It can be used for storage, processing, retrieval and pattern mining of data stored in a relational database format only It works well for structured and unstructured data. It works only for structured data only It can many technology stacks on top of it each doing a specific task like HDFS, AVRO, Pig, HBase etc. SQL is a query language with specific syntax and a scheme to get around with things Data can be stored in the form of key-value pairs, tables, hash map etc. Data is stored in the form of tables only It supports NoSQL type data structures, columnar data structures etc. like MongoDB It works on the property of ACID It can be used to store and process log data, real-time data, images, videos, sensor data and other variety of data. Data variety is severely restricted in SQL Hadoop is used mainly in those applications where data volume is huge and systems like SQL cannot perform well. SQL can store a moderate volume of data INSERT, SELECT type statements are very fast in Hadoop compared to SQL SQL syntax are much slower when executed on millions of rows at a time Hadoop uses the concept of distributed computing, applies the principle of map-reduce and thus handle data available on multiple systems across multiple locations. SQL data sources are usually available on-premise or on a cloud. Thus it cannot exploit the advantages of distributed computing Hadoop based systems can be easily and cost-effectively scaled. Horizontal scaling is very cheap and as many computers can be connected to the network as desired thus it is scalable on demand. Buying an additional SQL server costs a fortune. If a system runs out of storage, additional racks and servers need to be purchased and configured which is expensive and time-consuming It is highly faulted tolerant. It has good fault tolerance It uses commodity hardware. It uses propriety hardware It is a free and open source. Most of the SQL systems are licensed Advanced machine learning and artificial intelligence techniques can be build using Hadoop. Support for ML and AI is highly limited on SQL and only a few companies provide that Using appropriate JDBC connectors, Hadoop can communicate with SQL systems and move data in between. SQL systems can also read and write data to Hadoop infrastructure Cloudera, Horton work, AWS are some of the providers of Hadoop systems. Microsoft, Oracle, SAP etc. are some of the well-known industry leaders in SQL systems Last but not the least, the learning curve of Hadoop for entry-level professionals, as well as a seasoned professional, is moderately hard. Starting with SQL systems is much easier for even entry-level professionals","title":"Hadoop vs SQL"},{"location":"big_data/hadoop/","text":"Hadoop Framework : 1. Common Utilities: Also called the Hadoop common. These are nothing but the JAVA libraries, files, scripts, and utilities that are actually required by the other Hadoop components to perform. 2. HDFS: Hadoop Distributed File System Solves the problem of storing big data. Why Hadoop has chosen to incorporate a Distributed file system? Let\u2019s understand this with an example: We need to read 1TB of data and we have one machine with 4 I/O channels each channel having 100MB/s, it took 45 minutes to read the entire data. Now the same amount of data is read by 10 machines each with 4 I/O channels each channel having 100MB/s. Guess the amount of time it took to read the data? 4.3 minutes. The two main components of HDFS are: NAME NODE. Name node is the master, We may have a secondary name node as well in case the primary name node stops working the secondary name node will act as a backup. The name node basically maintains and manages the data nodes by storing metadata. DATA NODE. The data node is the slave which is basically the low-cost commodity hardware. We can have multiple data nodes. The data node stores the actual data. This data node supports the replication factor, suppose if one data node goes down then the data can be accessed by the other replicated data node, therefore, the accessibility of data is improved and loss of data is prevented. 3. Map Reduce: Solves the problem of processing big data. Let\u2019s understand the concept of map reduces by solving this real-world problem. ABC company wants to calculate its total sales, city wise. Now here the hash table concept won\u2019t work because the data is in terabytes, so we will use the Map-Reduce concept. There are two phases: MAP First, we will split the data into smaller chunks called the mappers on the basis of the key/value pair. So here the key will be the city name and the value will be total sales. Each mapper will get each month\u2019s data which gives a city name and corresponding sales. REDUCE It will get these piles of data and each reducer will be responsible for North/West/East/South cities. So the work of the reducer will be collecting these small chunks and convert into larger amounts (by adding them up) for a particular city. 4. YARN Framework: It is the middle layer between HDFS and Map Reduce which is responsible for managing cluster resources. Yet another resource negotiator. The initial version of Hadoop had just two components: Map Reduce and HDFS. Later it was realized that Map Reduce couldn\u2019t solve a lot of big data problems. The idea was to take the resource management and job scheduling responsibilities away from the old map-reduce engine and give it to a new component. This is how YARN came into the picture. It is having two key roles to perform: Job Scheduling When a large amount of data is giving for processing, it needs to be distributed and broken down into different tasks/jobs. Now the JS decides which job needs to be given the top priority, the time interval between two jobs, dependency among the jobs, checks that there is no overlapping between the jobs running. Resource management For processing the data and for storing the data we need resources right? So the resource manager provides, manages and maintains the resources to store and process the data.","title":"Hadoop"},{"location":"big_data/hadoop/#hadoop-framework","text":"","title":"Hadoop Framework :"},{"location":"big_data/hadoop/#1-common-utilities","text":"Also called the Hadoop common. These are nothing but the JAVA libraries, files, scripts, and utilities that are actually required by the other Hadoop components to perform.","title":"1. Common Utilities:"},{"location":"big_data/hadoop/#2-hdfs-hadoop-distributed-file-system","text":"Solves the problem of storing big data. Why Hadoop has chosen to incorporate a Distributed file system? Let\u2019s understand this with an example: We need to read 1TB of data and we have one machine with 4 I/O channels each channel having 100MB/s, it took 45 minutes to read the entire data. Now the same amount of data is read by 10 machines each with 4 I/O channels each channel having 100MB/s. Guess the amount of time it took to read the data? 4.3 minutes. The two main components of HDFS are: NAME NODE. Name node is the master, We may have a secondary name node as well in case the primary name node stops working the secondary name node will act as a backup. The name node basically maintains and manages the data nodes by storing metadata. DATA NODE. The data node is the slave which is basically the low-cost commodity hardware. We can have multiple data nodes. The data node stores the actual data. This data node supports the replication factor, suppose if one data node goes down then the data can be accessed by the other replicated data node, therefore, the accessibility of data is improved and loss of data is prevented.","title":"2. HDFS: Hadoop Distributed File System"},{"location":"big_data/hadoop/#3-map-reduce","text":"Solves the problem of processing big data. Let\u2019s understand the concept of map reduces by solving this real-world problem. ABC company wants to calculate its total sales, city wise. Now here the hash table concept won\u2019t work because the data is in terabytes, so we will use the Map-Reduce concept. There are two phases: MAP First, we will split the data into smaller chunks called the mappers on the basis of the key/value pair. So here the key will be the city name and the value will be total sales. Each mapper will get each month\u2019s data which gives a city name and corresponding sales. REDUCE It will get these piles of data and each reducer will be responsible for North/West/East/South cities. So the work of the reducer will be collecting these small chunks and convert into larger amounts (by adding them up) for a particular city.","title":"3. Map Reduce:"},{"location":"big_data/hadoop/#4-yarn-framework","text":"It is the middle layer between HDFS and Map Reduce which is responsible for managing cluster resources. Yet another resource negotiator. The initial version of Hadoop had just two components: Map Reduce and HDFS. Later it was realized that Map Reduce couldn\u2019t solve a lot of big data problems. The idea was to take the resource management and job scheduling responsibilities away from the old map-reduce engine and give it to a new component. This is how YARN came into the picture. It is having two key roles to perform: Job Scheduling When a large amount of data is giving for processing, it needs to be distributed and broken down into different tasks/jobs. Now the JS decides which job needs to be given the top priority, the time interval between two jobs, dependency among the jobs, checks that there is no overlapping between the jobs running. Resource management For processing the data and for storing the data we need resources right? So the resource manager provides, manages and maintains the resources to store and process the data.","title":"4. YARN Framework:"},{"location":"big_data/hdfs/","text":"What is HDFS? HDFS stands for Hadoop Distributed File System, which is used in the Hadoop framework to store huge datasets that run on commodity hardware. It is the core component of Hadoop which stores a massive amount of data using inexpensive hardware. With the increase in the volume of data, Big Data technologies have helped organizations in tackling the problem of storing as well as processing the huge amount of data. Hadoop is a framework which both stores and processes the huge datasets. Understanding HDFS HDFS has services such as NameNode, DataNode, Job Tracker, Task Tracker, and Secondary Name Node. HDFS also provides by default 3 replications of data across the cluster which helps in retrieving the data if one node is down due to failure. For example, if there is one file with a size of 100 MB, this file gets stored across the HDFS in 3 replications taking up a total of 300 MB with the two extra files as back up. NameNode and Job Tracker are called Master Nodes whereas DataNode and Task Tracker are called Slave Nodes. The metadata gets stored in NameNode and the data gets stored in the blocks of different DataNodes based on the availability of free space across the cluster. If the metadata is lost, then HDFS will not work and as the NameNode saves the metadata, it should have highly reliable hardware. The Secondary NameNode acts as a standby node for NameNode during failure. If a DataNode fails, then the metadata of that DataNode is removed from the NameNode and the metadata of newly allocated DataNode instead of the failed one is taken by the NameNode. How does HDFS make Working so Easy? HDFS provides the feature of replicating the data among the DataNodes and in case of any failure in the cluster it is easy to keep the data safe as the Data becomes available on other Nodes. Also one does not need to have highly reliable hardware across the cluster. The DataNodes can be cheap hardware and only one highly reliable NameNode storing the metadata is required. What can you do with HDFS? One can build a robust system to store huge amount of data which is easy to retrieve and provides fault tolerance and scalability. It is easy to add hardware which is inexpensive and can be easily monitored through one of the slave services. Working with HDFS It is the backbone of Hadoop and provides many features to suit the needs of the Big Data environment. Working with HDFS makes it easier to handle large clusters and maintain them. It is easy to achieve scalability and fault tolerance through HDFS. Popular Course in this category Hadoop Certification Training (20 Courses, 14+ Projects) 20 Online Courses | 14 Hands-on Projects | 135+ Hours | Verifiable Certificate of Completion | Lifetime Access 4.5 (2,034 ratings)Course Price $99 $399 View Course Related Courses MapReduce Training (2 Courses, 4+ Projects)Splunk Training Certification (4 Courses, 7+ Projects)Apache Pig Training (2 Courses, 4+ Projects) Advantages One of the advantages of using HDFS is its cost-effectiveness. Organizations can build a reliable system with inexpensive hardware for storage and it works well with Map Reduce, which is the processing model of Hadoop. It is efficient in performing sequential reads and writes which is the access pattern in Map Reduce Jobs. Required HDFS Skills As HDFS is designed for Hadoop Framework, knowledge of Hadoop Architecture is vital. Also, the Hadoop framework is written in JAVA, so a good understanding of JAVA programming is very crucial. It is used along with Map Reduce Model, so a good understanding of Map Reduce job is an added bonus. Apart from above, a good understanding of Database, practical knowledge of Hive Query Language along with problem-solving and analytical skill in Big Data environment are required. Why should we use HDFS? With the increase in data volume every second, the need to store the huge amount of data which can be up to Terabytes in size and having a fault tolerant system has made HDFS popular for many organizations. HDFS stores the files in blocks and provides replication. The unused space in a block can be used for storing other data. NameNode stores the metadata, so it has to be highly reliable. But the DataNodes storing the actual data are inexpensive hardware. So because of two of its most prominent advantages, it is highly recommended and trusted. Scope The amount of data produced from unnumbered sources is massive, which makes the analysis and storage even more difficult. For solving these Big Data problems, Hadoop has become so popular with its two components, HDFS and Map Reduce. As the data grows every second of every day, the need for technologies like HDFS even grows more as the organizations cannot just ignore the massive amount of data. Why do we need HDFS? Organizations are rapidly moving towards a direction where data has utmost importance. The Data gathered from many sources and also data generated by their Businesses every day are equally important. So adopting a model like HDFS may suit very well to their needs along with reliability.","title":"Hdfs"},{"location":"big_data/pig/","text":"Difference Between Apache Pig and Apache Hive The Apache Pig story begins in the year 2006 when the researcher as Yahoo was struggling with MapReduce Java codes. It was difficult to reuse and maintain code for compilation. At the same time, they observed that MapReduce users were not comfortable with declarative languages such as SQL. They started to work on new language that was supposed to fit in a sweet spot between the declarative style of SQL, low-level and procedural style of MapReduce. This resulted in the birth of Pig and the first release of Pig came in September 2008 and by end of 2009 about half of the jobs at Yahoo were Pig jobs. The Apache Hive story begins in the year 2007 when non-Java Programmer have to struggle while using Hadoop MapReduce. IT professional from database background were facing challenges to work on Hadoop Cluster. Initially, researchers, working at Facebook came up with Hive language. This language was very similar to SQL language. So language was called Hive Query Language (HQL) and later it becomes project of open source Apache Community. After becoming project of Apache Community there was a major development in Apache Hive. Facebook was the first company to come up with Apache Hive. Let me explain about Apache Pig vs Apache Hive in more details. Introducing Apache Pig vs Apache Hive Apache Pig is a platform for analyzing large data sets that consists of a high-level language for expressing data analysis programs, coupled with infrastructure for evaluating these programs. Apache is open source project of Apache Community. Apache Pig provides a simple language called Pig Latin, for queries and data manipulation. Pig is being utilized by companies like Yahoo, Google and Microsoft for collecting huge amounts of data sets in the form of click streams, search logs and web crawls. Apache Pig provides nested data types like Maps, Tuples, and Bags Apache Pig Follows multi-query approach to avoid multiple scans of the datasets. Programmers familiar with scripting language prefer Apache Pig Pig is easy if you are well aware of SQL No need to create schema to work on Apache Pig Pig also provides support to major data operations like Ordering, Filters, and Joins Apache Pig framework translates Pig Latin into sequences of MapReduce programs Apache Hive data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Apache Hive is an Apache open-source project built on top of Hadoop for querying, summarizing and analyzing large data sets using a SQL-like interface. Apache hive provides the SQL-like language called HiveQL, which transparently convert queries to MapReduce for execution on large datasets stored in Hadoop Distributed File System (HDFS). Apache Hive is a Data warehouse Infrastructure. Apache Hive is an ETL tool (Extraction-Transformation-Loading) Apache hive is similar to SQL Apache Hive enables customized mappers and reducers Apache Hive increases the schema design flexibility using data serialization and deserialization Apache hive is an analytical tool Key differences between Apache Pig vs Apache Hive: Apache Pig is more faster comparing Apache Hive Apache Pig and Apache Hive both runs on top of Hadoop MapReduce Apache Pig is best for Structured and Semi-structured while Apache Hive is best for structured data Apache Pig is a procedural language while Apache Hive is a declarative language Apache Pig supports cogroup feature for outer joins while Apache Hive does not support Apache Pig does not have a pre-defined database to store table/ schema while Apache Hive has pre-defined tables/schema and stores its information in a database. Apache Pig is also suited for complex and nested data structure while Apache Hive is less suited for complex data Researchers and programmers use Apache pig while Data Analysts use Apache Hive When to use Apache Pig: When you are a programmer and know scripting language When you don\u2019t want to create schema while loading ETL requirements When you are working on client side of the Hadoop cluster When you are working on Avro Hadoop file format When to use Apache Hive: Data warehousing requirements Analytical Queries of historical data Data Analysis who are familiar with SQL While working on structured data By Data Analysts To visualize and create reports Apache Pig vs Apache Hive Comparison Table I am discussing major artifacts and distinguishing between Apache Pig and Apache Hive. Apache Pig Apache Hive Data Processing Apache Pig is High-level data flow language Apache Hive is used for batch processing i.e. Online Analytical Processing (OLAP) Processing Speed Apache Pig has higher latency because of executing MapReduce job in background Apache Hive also has higher latency because of executing MapReduce job in background Compatibility with Hadoop Apache Pig runs on top of MapReduce Apache Hive also runs on top of MapReduce Definition Apache Pig is open source, high-level data flow system that renders you a simple language platform properly known as Pig Latin that can be used for manipulating data and queries. Apache Hive is open source and similar to SQL used for Analytical Queries Language Used Apache Pig uses procedural data flow language called Pig Latin Apache Hive uses a declarative language called HiveQL Schema Apache Pig doesn\u2019t have a concept of schema. You can store data in an alias. Apache hive supports Schema for inserting data in tables Web Interface Apache Pig does not support web Interface Apache Hive supports web interface Operations Apache Pig is used for Structured and Semi-Structured data Apache Hive is used for structured data. User Specification Apache Pig is used by Researchers and Programmers Apache Hive is used by Data Analyst Operates On Apache Pig operates on Client side of cluster Apache hive Operates on Server side of Cluster Partition Methods There is no concept of Partition in Apache Pig Apache Hive supports Sharding features File Format Apache Pig Supports Avro file format Apache hive directly does not support Avro format but can support using \u201corg.apache.hadoop.hive.serde2.avro\u201d JDBC / ODBC Apache Pig does not support Apache hive supports but limited Debugging It is easy to debug Pig scripts We can debug, but it is bit complex","title":"Pig"},{"location":"big_data/searching/","text":"Searching Techniques Searching is an operation or a technique that helps finds the place of a given element or value in the list. Any search is said to be successful or unsuccessful depending upon whether the element that is being searched is found or not. Some of the standard searching technique that is being followed in the data structure is listed below: Linear Search This is the simplest method for searching. In this technique of searching, the element to be found in searching the elements to be found is searched sequentially in the list. This method can be performed on a sorted or an unsorted list (usually arrays). In case of a sorted list searching starts from 0th element and continues until the element is found from the list or the element whose value is greater than (assuming the list is sorted in ascending order), the value being searched is reached. As against this, searching in case of unsorted list also begins from the 0th element and continues until the element or the end of the list is reached. Algorithm: It is a simple algorithm that searches for a specific item inside a list. It operates looping on each element O(n) unless and until a match occurs or the end of the array is reached. algorithm Seqnl_Search(list, item) Pre: list != ; Post: return the index of the item if found, otherwise: 1 index <- fi while index < list.Cnt and list[index] != item //cnt: counter variable index <- index + 1 end while if index < list.Cnt and list[index] = item return index end if return: 1 end Seqnl_Search Binary Search Binary search is a very fast and efficient searching technique. It requires the list to be in sorted order. In this method, to search an element you can compare it with the present element at the center of the list. If it matches, then the search is successful otherwise the list is divided into two halves: one from the 0th element to the middle element which is the center element (first half) another from the center element to the last element (which is the 2nd half) where all values are greater than the center element. The searching mechanism proceeds from either of the two halves depending upon whether the target element is greater or smaller than the central element. If the element is smaller than the central element, then searching is done in the first half, otherwise searching is done in the second half. Algorithm: algorithm Binary_Search(list, item) Set L to 0 and R to n: 1 if L > R, then Binary_Search terminates as unsuccessful else Set m (the position in the mid element) to the floor of (L + R) / 2 if Am < T, set L to m + 1 and go to step 3 if Am > T, set R to m: 1 and go to step 3 Now, Am = T, the search is done; return (m)","title":"Searching"},{"location":"big_data/searching/#searching-techniques","text":"Searching is an operation or a technique that helps finds the place of a given element or value in the list. Any search is said to be successful or unsuccessful depending upon whether the element that is being searched is found or not. Some of the standard searching technique that is being followed in the data structure is listed below: Linear Search This is the simplest method for searching. In this technique of searching, the element to be found in searching the elements to be found is searched sequentially in the list. This method can be performed on a sorted or an unsorted list (usually arrays). In case of a sorted list searching starts from 0th element and continues until the element is found from the list or the element whose value is greater than (assuming the list is sorted in ascending order), the value being searched is reached. As against this, searching in case of unsorted list also begins from the 0th element and continues until the element or the end of the list is reached. Algorithm: It is a simple algorithm that searches for a specific item inside a list. It operates looping on each element O(n) unless and until a match occurs or the end of the array is reached. algorithm Seqnl_Search(list, item) Pre: list != ; Post: return the index of the item if found, otherwise: 1 index <- fi while index < list.Cnt and list[index] != item //cnt: counter variable index <- index + 1 end while if index < list.Cnt and list[index] = item return index end if return: 1 end Seqnl_Search Binary Search Binary search is a very fast and efficient searching technique. It requires the list to be in sorted order. In this method, to search an element you can compare it with the present element at the center of the list. If it matches, then the search is successful otherwise the list is divided into two halves: one from the 0th element to the middle element which is the center element (first half) another from the center element to the last element (which is the 2nd half) where all values are greater than the center element. The searching mechanism proceeds from either of the two halves depending upon whether the target element is greater or smaller than the central element. If the element is smaller than the central element, then searching is done in the first half, otherwise searching is done in the second half. Algorithm: algorithm Binary_Search(list, item) Set L to 0 and R to n: 1 if L > R, then Binary_Search terminates as unsuccessful else Set m (the position in the mid element) to the floor of (L + R) / 2 if Am < T, set L to m + 1 and go to step 3 if Am > T, set R to m: 1 and go to step 3 Now, Am = T, the search is done; return (m)","title":"Searching Techniques"},{"location":"big_data/sorting/","text":"The techniques of sorting can be divided into two categories. These are: Internal Sorting: If all the data that is to be sorted can be adjusted at a time in the main memory, the internal sorting method is being performed. External Sorting: When the data that is to be sorted cannot be accommodated in the memory at the same time and some has to be kept in auxiliary memory such as hard disk, floppy disk, magnetic tapes etc, then external sorting methods are performed. In data processing, there are various sorting methods and techniques that are not only used for sorting algorithms but are also used for analyzing the performance of other algorithms. In this post, you will find a brief description of the different types of sorting algorithms. Sorting algorithms can be categorized as Simple Sorts : These types of algorithms are efficient on the small amount of data but cannot handle large data. They are fast and efficient due to low overhead. Two simplest sort algorithms are insertion sort and selection sorts Effecient Sorts : Practical sorting algorithms are usually based on algorithms with average time complexity. Some most common of these are merge sort, heap sort, and quicksort. These algorithms can be used on large lists and complicated programs but each of them has its own drawbacks and advantages. Some may require additional space or more iterations, thus resulting in more complex algorithms. Examples: MergeSort: Compares two elements of the list and then swaps them in the order required (ascending or descending). It applies the divide and rule concept. Merge sort works on sequential access and can work on large lists. This algorithm is popularly used in practical programming as it is used in the sophisticated algorithm Timsort. Timsort is used for standard sorting in languages such as Python and Jana. Merge sort algorithm is itself a standard routine in Perl. Algorithm: Divide the list recursively into two or more sub-problems until it can no more be divided Solve the sub-problems until it is reached to the base case Merge the smaller lists into the new list in sorted order HeapSort: An advanced and efficient version of the selection sort algorithm. It works similarly by sorting the elements in the ascending or descending order by comparing but this is done by using a data structure called heap, which is a special tree base structure. Heap has the following properties: It is a complete tree Root has a greater value than any other element in the subtree Algorithm: Form a heap from the given data Remove the largest item from the heap From the remaining data reform the heap Repeat step 2 and 3 until all the data is over QuickSort: Similar to merge sort which uses divide and conquers technique. Recursive algorithm. But quicksort performs in a little different manner than mergesort does. In merge sort, the work does not happen in the division stage but it happens in the combined stage. But in quicksort it is totally opposite, everything happens in the division stage. Algorithm: Divide the list by any chosen element and call it a Pivot element. Rearrange the elements, all the elements that are less than equal to the pivot element must be in left and larger ones on the right. This procedure is called Partitioning. Conquer which mean recursively ordering the subarrays There is nothing left in the combined stage as the conquer stage organizes everything. The smaller or equal elements to the pivot are organized at the left side and larger elements are organized at the right.","title":"Sorting"},{"location":"big_data/sorting_brilliant/","text":"A sorting algorithm is an algorithm made up of a series of instructions that takes an array as input, performs specified operations on the array, sometimes called a list, and outputs a sorted array. Sorting algorithms are often taught early in computer science classes as they provide a straightforward way to introduce other key computer science topics like Big-O notation, divide-and-conquer methods, and data structures such as binary trees, and heaps. There are many factors to consider when choosing a sorting algorithm to use. Sorting Algorithms In other words, a sorted array is an array that is in a particular order. For example, [a,b,c,d][a,b,c,d] is sorted alphabetically, [1,2,3,4,5][1,2,3,4,5] is a list of integers sorted in increasing order, and [5,4,3,2,1][5,4,3,2,1] is a list of integers sorted in decreasing order. A sorting algorithm takes an array as input and outputs a permutation of that array that is sorted. There are two broad types of sorting algorithms: integer sorts and comparison sorts. Comparison Sorts Comparison sorts compare elements at each step of the algorithm to determine if one element should be to the left or right of another element. Comparison sorts are usually more straightforward to implement than integer sorts, but comparison sorts are limited by a lower bound of O(n \\log n)O(nlogn), meaning that, on average, comparison sorts cannot be faster than O(n \\log n)O(nlogn). A lower bound for an algorithm is the worst-case running time of the best possible algorithm for a given problem. The \"on average\" part here is important: there are many algorithms that run in very fast time if the inputted list is already sorted, or has some very particular (and overall unlikely) property. There is only one permutation of a list that is sorted, but n!n! possible lists, so the chances that the input is already sorted is very unlikely, and on average, the list will not be very sorted. The running time of comparison-based sorting algorithms is bounded by \\Omega(n \\log n)\u03a9(nlogn). A comparison sort can be modeled as a large binary tree called a decision tree where each node represents a single comparison. Because the sorted list is some permutation of the input list, for an input list of length nn, there are n!n! possible permutations of that list. This is a decision tree because each of the n!n! is represented by a leaf, and the path the algorithm must take to get to each leaf is the series of comparisons and outcomes that yield that particular ordering. At each level of the tree, a comparison is made. Comparisons happen, and we keep traveling down the tree; until the algorithm reaches the leaves of the tree, there will be a leaf for each permutation, so there are n!n! leaves. Each comparison halves the number of future comparisons the algorithm must do (since if the algorithm selects the right edge out of a node at a given step, it will not search the nodes and paths connected to the left edge). Therefore, the algorithm performs O(\\log n!)O(logn!) comparisons. Any binary tree, with height hh, has a number of leaves that is less than or equal to 2^h2 h . From this, 2^h \\geq n!. 2 h \u2265n!. Taking the logarithm results in h \\geq \\log(n!). h\u2265log(n!). From Stirling's approximation, n! > \\left(\\frac{n}{e}\\right)^n. n!>( e n \u200b | ) n . Therefore, \\begin{aligned} h &\\geq \\log\\left(\\frac{n}{e}\\right)^n \\ &= n\\log \\left(\\frac{n}{e}\\right) \\ &= n\\log n- n \\log e\\ &= \\Omega(n\\log n). \\end{aligned} h \u200b | \u2265log( e n \u200b | ) n =nlog( e n \u200b | ) =nlogn\u2212nloge =\u03a9(nlogn). \u200b | Integer Sorts Integer sorts are sometimes called counting sorts (though there is a specific integer sort algorithm called counting sort). Integer sorts do not make comparisons, so they are not bounded by \\Omega(n\\log n)\u03a9(nlogn). Integer sorts determine for each element\u200b xx how many elements are less than xx. If there are 1414 elements that are less than xx, then xx will be placed in the 15^\\text{th}15 th slot. This information is used to place each element into the correct slot immediately\u2014no need to rearrange lists. Properties of Sorting Algorithms All sorting algorithms share the goal of outputting a sorted list, but the way that each algorithm goes about this task can vary. When working with any kind of algorithm, it is important to know how fast it runs and in how much space it operates\u2014in other words, its time complexity and space complexity. As shown in the section above, comparison-based sorting algorithms have a time complexity of \\Omega(n\\log n)\u03a9(nlogn), meaning the algorithm can't be faster than n \\log nnlogn. However, usually, the running time of algorithms is discussed\u200b in terms of big O, and not Omega. For example, if an algorithm had a worst-case running time of O(n\\log n)O(nlogn), then it is guaranteed that the algorithm will never be slower than O(n\\log n)O(nlogn), and if an algorithm has an average-case running time of O(n^2)O(n 2 ), then on average, it will not be slower than O(n^2)O(n 2 ). The running time describes how many operations an algorithm must carry out before it completes. The space complexity describes how much space must be allocated to run a particular algorithm. For example, if an algorithm takes in a list of size nn, and for some reason makes a new list of size nn for each element in nn, the algorithm needs n^2n 2 space. O(1) O(1) O(n) O(n) O\\big(n^2\\big) O(n 2 ) O(n\\log n) O(nlogn) Find the big-O running time of a sorting program that does the following: It takes in a list of integers. It iterates once through the list to find the largest element, and moves that element to the end. It repeatedly finds the largest element in the unsorted portion by iterating once through, and moves that element to the end of the unsorted portion. At the end, the list is sorted low to high. (Also, try implementing this program in your language of choice.) Additionally, for sorting algorithms, it is sometimes useful to know if a sorting algorithm is stable. Stability A sorting algorithm is stable if it preserves the original order of elements with equal key values (where the key is the value the algorithm sorts by). For example, [1] When the cards are sorted by value with a stable sort, the two 5s must remain in the same order in the sorted output that they were originally in. When they are sorted with a non-stable sort, the 5s may end up in the opposite order in the sorted output. Choosing a Sorting Algorithm To choose a sorting algorithm for a particular problem, consider the running time, space complexity, and the expected format of the input list. Algorithm Best-case Worst-case Average-case Space Complexity Stable? Merge Sort O(n \\log n)O(nlogn) O(n \\log n)O(nlogn) O(n \\log n)O(nlogn) O(n)O(n) Yes Insertion Sort O(n)O(n) O(n^2)O(n2) O(n^2)O(n2) O(1)O(1) Yes Bubble Sort O(n)O(n) O(n^2)O(n2) O(n^2)O(n2 O(1)O(1) Yes Quicksort O(n \\log n)O(nlogn) O(n^2)O(n2) O(n \\log n)O(nlogn) \\log nlogn best, nn avg Usually not* Heapsort O(n log n)O(nlogn) O(n \\log n)O(nlogn) O(n \\log n)O(nlogn) O(1)O(1) No Counting Sort O(k+n)O(k+n) O(k+n)O(k+n) O(k+n)O(k+n) O(k+n)O(k+n) Yes *Most quicksort implementations are not stable, though stable implementations do exist. When choosing a sorting algorithm to use, weigh these factors. For example, quicksort is a very fast algorithm but can be pretty tricky to implement; bubble sort is a slow algorithm but is very easy to implement. To sort small sets of data, bubble sort may be a better option since it can be implemented quickly, but for larger datasets, the speedup from quicksort might be worth the trouble implementing the algorithm subscript and superscript","title":"Sorting brilliant"},{"location":"big_data/sorts_/","text":"Sorts Comparison sorts\u200e (30 P) A comparison sort is a particular type of sorting algorithm which can only read the list elements through a single abstract comparison operation (often a \"less than\" operator) that determines which of two elements should occur first in the final sorted list. Adaptive heap sort Block sort Bogosort Bubble sort Cache-oblivious distribution sort Cascade merge sort Cocktail shaker sort Comb sort Cubesort Cycle sort Funnelsort Gnome sort Heapsort Insertion sort Introsort Library sort Merge sort Multi-key quicksort Odd\u2013even sort Oscillating merge sort Patience sorting Polyphase merge sort Quicksort Selection sort Shellsort Smoothsort Sorting number Stooge sort Timsort Online sorts\u200e (6 P) These sorts can start sorting their input without having received all of it, and are therefore classified as online algorithms. Selection algorithms\u200e (7 P) In computer science, a selection algorithm is an algorithm for finding the kth smallest number in a list or array; such a number is called the kth order statistic. This includes the cases of finding the minimum, maximum, and median elements. There are O(n)-time (worst-case linear time) selection algorithms, and sublinear performance is possible for structured data; in the extreme, O(1) for an array of sorted data. Selection is a subproblem of more complex problems like the nearest neighbor and shortest path problems. Many selection algorithms are derived by generalizing a sorting algorithm, and conversely some sorting algorithms can be derived as repeated application of selection. BFPRT Floyd\u2013Rivest algorithm Introselect Median of medians Order statistic tree Quickselect Stable sorts\u200e (18 P) String sorting algorithms\u200e (4 P)","title":"Sorts "},{"location":"big_data/sorts_/#sorts","text":"Comparison sorts\u200e (30 P) A comparison sort is a particular type of sorting algorithm which can only read the list elements through a single abstract comparison operation (often a \"less than\" operator) that determines which of two elements should occur first in the final sorted list. Adaptive heap sort Block sort Bogosort Bubble sort Cache-oblivious distribution sort Cascade merge sort Cocktail shaker sort Comb sort Cubesort Cycle sort Funnelsort Gnome sort Heapsort Insertion sort Introsort Library sort Merge sort Multi-key quicksort Odd\u2013even sort Oscillating merge sort Patience sorting Polyphase merge sort Quicksort Selection sort Shellsort Smoothsort Sorting number Stooge sort Timsort Online sorts\u200e (6 P) These sorts can start sorting their input without having received all of it, and are therefore classified as online algorithms. Selection algorithms\u200e (7 P) In computer science, a selection algorithm is an algorithm for finding the kth smallest number in a list or array; such a number is called the kth order statistic. This includes the cases of finding the minimum, maximum, and median elements. There are O(n)-time (worst-case linear time) selection algorithms, and sublinear performance is possible for structured data; in the extreme, O(1) for an array of sorted data. Selection is a subproblem of more complex problems like the nearest neighbor and shortest path problems. Many selection algorithms are derived by generalizing a sorting algorithm, and conversely some sorting algorithms can be derived as repeated application of selection. BFPRT Floyd\u2013Rivest algorithm Introselect Median of medians Order statistic tree Quickselect Stable sorts\u200e (18 P) String sorting algorithms\u200e (4 P)","title":"Sorts"},{"location":"big_data/splunk/","text":"Introduction To Uses of Splunk Splunk is a software that provides you with an engine that helps in monitoring, searching, analyzing, visualizing and which acts on large amounts of data. It is a wide application and it supports and works on versatile technologies. Splunk is an advanced technology which searches log files which are stored in a system. It also helps in operational intelligence. Splunk has many uses and it does not require any complicated databases, connectors or controls. It can also be used as a cloud application which is highly scalable and reliable. Top 10 Uses of Splunk: Below is the list of top 10 uses of Splunk are as follows: Search Processing Language Splunk provides a search processing language which enables searching easily. This language is extremely powerful for scrutinizing through large amounts of data and performing statistical operations for any specific context. You can consider an example where you may want to get the information of applications that are slowest to start up and as a result making the user wait for the longest. If you enter the following words in Splunk you will get the required results. Searching a particular data is easy and can be easily searched by entering below: index=uberagent sourcetype=uberAgent:Process:ProcessStartup | timechart avg(StartupTimeMs) by Name This provides the exact results from the log without much efforts for searching for them. This makes it much more effective. It provides a variety of Apps, Add-ons and Data sources For Splunk to find out the duration on when the application is starting or how much a user is waiting is from the data it receives from a variety of sources. These sources can be any from all kinds of log files, Windows event logs, Syslog, SNMP, to name a few. You gave the facility of looking for data by writing a script and directing it to Splunk. Even after this if you are unable to find what you need you to have Splunk\u2019s App Directory as an add-in that helps in collecting the necessary data. All data that comes can be having vast limits and may have user experiences and application monitoring, agents. This data is monitored at different endpoints and independently of Splunk and sends the data that it collects in Splunk and processes it further. Splunk apps can be data inputs and they also provide dashboards that visualize what Splunk has indexed. Popular Course in this category Hadoop Certification Training (20 Courses, 14+ Projects) 20 Online Courses | 14 Hands-on Projects | 135+ Hours | Verifiable Certificate of Completion | Lifetime Access 4.5 (2,034 ratings)Course Price $99 $399 View Course Related Courses MapReduce Training (2 Courses, 4+ Projects)Splunk Training Certification (4 Courses, 7+ Projects)Apache Pig Training (2 Courses, 4+ Projects) 3. Indexes and Events Splunk accepts all data immediately after installation. It does not have any fixed schema and takes all data as it is. When it starts searching the data at that time it performs field extraction. Mostly all log formats are recognized automatically and everything else can be specified in configuration files. This helps in bringing flexibility. uses of Splunk can take any kind of data present in the system and create its index. When indexing is done it processes incoming data and prepares it for storage. All data is segregated and streams of characters are created as individual events. It is Scalable and has no Backend There is no backend to manage or any database to set up when Splunk comes into the picture. This makes Splunk available on multiple platforms and can be installed speedily on any software. If one server is not enough another can be added easily and data is distributed across both these servers evenly. This increases the speed with the number of machines that is holding the data. As this is distributed over many environments there is no single point of failure. Reporting and Alerting uses of Splunk can generate a variety of reports like graphs, pie charts, bar charts, etc. The tools that it uses to generate these reports are great. From statistics to frequencies to correlations, everything can be captured in a report. Each report has a dashboard and gives the viewer many options for customizing and bringing out the necessary data with the changing timeframes and data sources. In addition to this is also has an alerting mechanism which helps in log management. These alerts are generated when Splunk queries are run and there are alerts and dependencies to be defined. These alerts can be sent over an email, RSS feeds or simply through a script. Monitoring and Diagnosis made easy In today\u2019s world of DevOps, it is sometimes difficult to check the underlying infrastructure and to quickly identify the root cause of issues. uses of Splunk provides visibility for a performance of the system and helps the customers to find problems and discover trends. Monitoring is much easier by looking at the indexes. All logs are generated and stored. Troubleshooting made easier With the log files which are stored in Splunk, it is easy to troubleshoot any issue that occurs. Splunk supports many configurations. To figure out which configuration is currently running is difficult. To make this easier there is a tool. This tool can help[ the user detect configuration file issues and see the current configurations that are being used. Btool displays merged on disk configurations and help in troubleshooting files issues or check the values being used by Splunk. Analyze system performance A user can monitor servers or Windows infrastructure by uses of Splunk. Performance monitoring covers dashboards for CPU, Memory, Physical Disk and Logical Disk, Network Interface, and System metrics. Each drop down also has text boxes where you can click and enter the required text. For Windows, this app immediately filters the collected metrics and shows entries that match your search. Dashboards to visualize and analyze results Splunk helps in the creation of different dashboards that help in better management of the system. It gives all different metrics a different dashboard. As a result, the data is segregated and can be managed well. Store and retrieve data Using the indexing and events data is stored in Splunk and can be used anytime. Whenever it is searched it can be fetched from there logs can be monitored easily.","title":"Splunk"},{"location":"big_data/ssssso/","text":"orting algorithms are often classified by: [[Computational complexity theory|Computational complexity]] ([[Best, worst and average case|worst, average and best]] behavior) in terms of the size of the list (''n''). For typical serial sorting algorithms good behavior is O(''n'' log ''n''), with parallel sort in O(log 2 ''n''), and bad behavior is O(''n'' 2 ). (See [[Big O notation]].) Ideal behavior for a serial sort is O(''n''), but this is not possible in the average case. Optimal parallel sorting is O(log ''n''). [[Comparison sort|Comparison-based sorting algorithms]] need at least \u03a9(''n'' log ''n'') comparisons for most inputs. [[Computational complexity theory|Computational complexity]] of swaps (for \"in-place\" algorithms). [[Memory (computing)|Memory]] usage (and use of other computer resources). In particular, some sorting algorithms are \"[[In-place algorithm|in-place]]\". Strictly, an in-place sort needs only O(1) memory beyond the items being sorted; sometimes O(log(''n'')) additional memory is considered \"in-place\". Recursion. Some algorithms are either recursive or non-recursive, while others may be both (e.g., merge sort). Stability: [[#Stability|stable sorting algorithms]] maintain the relative order of records with equal keys (i.e., values). Whether or not they are a [[comparison sort]]. A comparison sort examines the data only by comparing two elements with a comparison operator. General method: insertion, exchange, selection, merging, ''etc.'' Exchange sorts include bubble sort and quicksort. Selection sorts include shaker sort and heapsort. Whether the algorithm is serial or parallel. The remainder of this discussion almost exclusively concentrates upon serial algorithms and assumes serial operation. Adaptability: Whether or not the presortedness of the input affects the running time. Algorithms that take this into account are known to be [[Adaptive sort|adaptive]]. ===Stability=== [[File:Sorting stability playing cards.svg|thumb|An example of stable sort on playing cards. When the cards are sorted by rank with a stable sort, the two 5s must remain in the same order in the sorted output that they were originally in. When they are sorted with a non-stable sort, the 5s may end up in the opposite order in the sorted output.]] Stable sort algorithms sort repeated elements in the same order that they appear in the input. When sorting some kinds of data, only part of the data is examined when determining the sort order. For example, in the card sorting example to the right, the cards are being sorted by their rank, and their suit is being ignored. This allows the possibility of multiple different correctly sorted versions of the original list. Stable sorting algorithms choose one of these, according to the following rule: if two items compare as equal, like the two 5 cards, then their relative order will be preserved, so that if one came before the other in the input, it will also come before the other in the output. Stability is important for the following reason: say, if the data is sorted first by student name, in some cases, dynamically on the webpage, and now the data is again sorted by which class section they are in. Imagine for students that appear in the same section, the order of their names is shuffled up and not in any particular order, and this can be annoying. If a sorting algorithm is stable, the student names will still be in good order. A user might want to have the previous chosen sort orders preserved on the screen and a stable sort algorithm can do that. Another reason why stability is important: if the users are not programmers, then they can choose to sort by section and then by name, by first sorting using name and then sort again using section. If the sort algorithm is not stable, the users won't be able to do that. More formally, the data being sorted can be represented as a record or tuple of values, and the part of the data that is used for sorting is called the ''key''. In the card example, cards are represented as a record (rank, suit), and the key is the rank. A sorting algorithm is stable if whenever there are two records R and S with the same key, and R appears before S in the original list, then R will always appear before S in the sorted list. When equal elements are indistinguishable, such as with integers, or more generally, any data where the entire element is the key, stability is not an issue. Stability is also not an issue if all keys are different. Unstable sorting algorithms can be specially implemented to be stable. One way of doing this is to artificially extend the key comparison, so that comparisons between two objects with otherwise equal keys are decided using the order of the entries in the original input list as a tie-breaker. Remembering this order, however, may require additional time and space. One application for stable sorting algorithms is sorting a list using a primary and secondary key. For example, suppose we wish to sort a hand of cards such that the suits are in the order clubs (\u2663), diamonds ( \u2666 ), hearts ( \u2665 ), spades (\u2660), and within each suit, the cards are sorted by rank. This can be done by first sorting the cards by rank (using any sort), and then doing a stable sort by suit: [[File:Sorting playing cards using stable sort.svg|400px]] Within each suit, the stable sort preserves the ordering by rank that was already done. This idea can be extended to any number of keys and is utilised by [[radix sort]]. The same effect can be achieved with an unstable sort by using a lexicographic key comparison, which, e.g., compares first by suit, and then compares by rank if the suits are the same.","title":"Ssssso"},{"location":"big_data/wwwi/","text":"Comparison sorts Name Best Average Worst Memory Stable Method Other notes Quicksort {\\displaystyle n\\log n}n\\log n variation is n {\\displaystyle n\\log n}n\\log n {\\displaystyle n^{2}}n^{2} {\\displaystyle \\log n}\\log n on average, worst case space complexity is n; Sedgewick variation is {\\displaystyle \\log n}\\log n worst case. Typical in-place sort is not stable; stable versions exist. Merge sort {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n n A hybrid block merge sort is O(1) mem. Yes Merging In-place merge sort \u2014 \u2014 {\\displaystyle n\\log ^{2}n}n\\log ^{2}n See above, for hybrid, that is {\\displaystyle n\\log n}n\\log n 1 Yes Merging Heapsort n If all keys are distinct, {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n 1 No Selection Insertion sort n {\\displaystyle n^{2}}n^{2} {\\displaystyle n^{2}}n^{2} 1 Yes Insertion O(n + d), in the worst case over sequences that have d inversions. Introsort {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n {\\displaystyle \\log n}\\log n No Partitioning & Selection Used in several STL implementations. Selection sort {\\displaystyle n^{2}}n^{2} {\\displaystyle n^{2}}n^{2} {\\displaystyle n^{2}}n^{2} 1 No Selection Stable with {\\displaystyle O(n)}O(n) extra space or when using linked lists.[9] Timsort n {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n n Yes Insertion & Merging Makes n comparisons when the data is already sorted or reverse sorted. Cubesort n {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n n Yes Insertion Makes n comparisons when the data is already sorted or reverse sorted. Shell sort {\\displaystyle n\\log n}n\\log n Depends on gap sequence Depends on gap sequence; best known is {\\displaystyle n^{4/3}}{\\displaystyle n^{4/3}} 1 No Insertion Bubble sort n {\\displaystyle n^{2}}n^{2} {\\displaystyle n^{2}}n^{2} 1 Yes Exchanging Tiny code size. Binary tree sort {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n(balanced) n Yes Insertion When using a self-balancing binary search tree. Cycle sort {\\displaystyle n^{2}}n^{2} {\\displaystyle n^{2}}n^{2} {\\displaystyle n^{2}}n^{2} 1 No Insertion In-place with theoretically optimal number of writes. Library sort n {\\displaystyle n\\log n}n\\log n {\\displaystyle n^{2}}n^{2} n Yes Insertion Patience sorting n \u2014 {\\displaystyle n\\log n}n\\log n n No Insertion & Selection Finds all the longest increasing subsequences in O(n log n). Smoothsort n {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n 1 No Selection An adaptive variant of heapsort based upon the Leonardo sequence rather than a traditional binary heap. Strand sort n {\\displaystyle n^{2}}n^{2} {\\displaystyle n^{2}}n^{2} n Yes Selection Tournament sort {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n n[10] No Selection Variation of Heap Sort. Cocktail sort n {\\displaystyle n^{2}}n^{2} {\\displaystyle n^{2}}n^{2} 1 Yes Exchanging Comb sort {\\displaystyle n\\log n}n\\log n {\\displaystyle n^{2}}n^{2} {\\displaystyle n^{2}}n^{2} 1 No Exchanging Faster than bubble sort on average. Gnome sort n {\\displaystyle n^{2}}n^{2} {\\displaystyle n^{2}}n^{2} 1 Yes Exchanging Tiny code size. UnShuffle Sort[11] n kn kn In-place for linked lists. n * sizeof(link) for array. n+1 for array? No Distribution and Merge No exchanges are performed. The parameter k is proportional to the entropy in the input. k = 1 for ordered or reverse ordered input. Franceschini's method[12] \u2014 {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n 1 Yes ? Block sort n {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n 1 Yes Insertion & Merging Combine a block-based {\\displaystyle O(n)}O(n) in-place merge algorithm[13] with a bottom-up merge sort. Odd\u2013even sort n {\\displaystyle n^{2}}n^{2} {\\displaystyle n^{2}}n^{2} 1 Yes Exchanging Can be run on parallel processors easily. Curve sort n {\\displaystyle n^{2}}n^{2} {\\displaystyle n^{2}}n^{2} {\\displaystyle \\Omega (1)\\cap {\\mathcal {O}}(n)}{\\displaystyle \\Omega (1)\\cap {\\mathcal {O}}(n)} Yes Insertion & counting Adapts to the smoothness of data. Non-comparison sorts The following table describes integer sorting algorithms and other sorting algorithms that are not comparison sorts. As such, they are not limited to \u03a9(n log n)[citation needed]. Complexities below assume n items to be sorted, with keys of size k, digit size d, and r the range of numbers to be sorted. Many of them are based on the assumption that the key size is large enough that all entries have unique key values, and hence that n \u226a 2k, where \u226a means \"much less than\". In the unit-cost random access machine model, algorithms with running time of {\\displaystyle \\scriptstyle n\\cdot {\\frac {k}{d}}}{\\displaystyle \\scriptstyle n\\cdot {\\frac {k}{d}}}, such as radix sort, still take time proportional to \u0398(n log n), because n is limited to be not more than {\\displaystyle 2^{\\frac {k}{d}}}2^{\\frac {k}{d}}, and a larger number of elements to sort would require a bigger k in order to store them in the memory.[14] Non-comparison sorts Name | Best | Average | Worst | Memory | Stable | n \u226a 2k | Notes Pigeonhole sort | \u2014 | {\\displaystyle n+2^{k}}n+2^{k} | {\\displaystyle n+2^{k}}n+2^{k} | {\\displaystyle 2^{k}}2^{k} | Yes | Yes | Bucket sort (uniform keys) | \u2014 | {\\displaystyle n+k}n+k | {\\displaystyle n^{2}\\cdot k}n^{2}\\cdot k | {\\displaystyle n\\cdot k}n\\cdot k | Yes | No | Assumes uniform distribution of elements from the domain in the array.[15] Bucket sort (integer keys) | \u2014 | {\\displaystyle n+r}n+r | {\\displaystyle n+r}n+r | {\\displaystyle n+r}n+r | Yes | Yes | If r is {\\displaystyle O(n)}O(n), then average time complexity is {\\displaystyle O(n)}O(n).[16] Counting sort | \u2014 | {\\displaystyle n+r}n+r | {\\displaystyle n+r}n+r | {\\displaystyle n+r}n+r | Yes | Yes | If r is {\\displaystyle O(n)}O(n), then average time complexity is {\\displaystyle O(n)}O(n).[15] LSD Radix Sort | \u2014 | {\\displaystyle n\\cdot {\\frac {k}{d}}}n\\cdot {\\frac {k}{d}} | {\\displaystyle n\\cdot {\\frac {k}{d}}}n\\cdot {\\frac {k}{d}} | {\\displaystyle n+2^{d}}n+2^{d} | Yes | No | ,[15][16] {\\displaystyle {\\frac {k}{d}}}{\\frac {k}{d}} recursion levels, 2d for count array. MSD Radix Sort | \u2014 | {\\displaystyle n\\cdot {\\frac {k}{d}}}n\\cdot {\\frac {k}{d}} | {\\displaystyle n\\cdot {\\frac {k}{d}}}n\\cdot {\\frac {k}{d}} | {\\displaystyle n+2^{d}}n+2^{d} | Yes | No | Stable version uses an external array of size n to hold all of the bins. MSD Radix Sort (in-place) | \u2014 | {\\displaystyle n\\cdot {\\frac {k}{d}}}n\\cdot {\\frac {k}{d}} | {\\displaystyle n\\cdot {\\frac {k}{d}}}n\\cdot {\\frac {k}{d}} | {\\displaystyle 2^{d}}2^{d} | No | No | d=1 for in-place, {\\displaystyle k/1}{\\displaystyle k/1} recursion levels, no count array. Spreadsort | n | {\\displaystyle n\\cdot {\\frac {k}{d}}}n\\cdot {\\frac {k}{d}} | {\\displaystyle n\\cdot \\left({{\\frac {k}{s}}+d}\\right)}n\\cdot \\left({{\\frac {k}{s}}+d}\\right) | {\\displaystyle {\\frac {k}{d}}\\cdot 2^{d}}{\\frac {k}{d}}\\cdot 2^{d} | No | No | Asymptotic are based on the assumption that n \u226a 2k, but the algorithm does not require this. Burstsort | \u2014 | {\\displaystyle n\\cdot {\\frac {k}{d}}}n\\cdot {\\frac {k}{d}} | {\\displaystyle n\\cdot {\\frac {k}{d}}}n\\cdot {\\frac {k}{d}} | {\\displaystyle n\\cdot {\\frac {k}{d}}}n\\cdot {\\frac {k}{d}} | No | No | Has better constant factor than radix sort for sorting strings. Though relies somewhat on specifics of commonly encountered strings. Flashsort | n | {\\displaystyle n+r}n+r | {\\displaystyle n^{2}}n^{2} | n | No | No | Requires uniform distribution of elements from the domain in the array to run in linear time. If distribution is extremely skewed then it can go quadratic if underlying sort is quadratic (it is usually an insertion sort). In-place version is not stable. Postman sort | \u2014 | {\\displaystyle n\\cdot {\\frac {k}{d}}}n\\cdot {\\frac {k}{d}} | {\\displaystyle n\\cdot {\\frac {k}{d}}}n\\cdot {\\frac {k}{d}} | {\\displaystyle n+2^{d}}n+2^{d} | \u2014 | No | A variation of bucket sort, which works very similar to MSD Radix Sort. Specific to post service needs. Samplesort can be used to parallelize any of the non-comparison sorts, by efficiently distributing data into several buckets and then passing down sorting to several processors, with no need to merge as buckets are already sorted between each other. Others Some algorithms are slow compared to those discussed above, such as the bogosort with unbounded run time and the stooge sort which has O(n2.7) run time. These sorts are usually described for educational purposes in order to demonstrate how run time of algorithms is estimated. The following table describes some sorting algorithms that are impractical for real-life use in traditional software contexts due to extremely poor performance or specialized hardware requirements. Name | Best | Average | Worst | Memory | Stable | Comparison | Other notes Bead sort | n | S | S | {\\displaystyle n^{2}}n^{2} | N/A | No | Works only with positive integers. Requires specialized hardware for it to run in guaranteed {\\displaystyle O(n)}O(n) time. There is a possibility for software implementation, but running time will be {\\displaystyle O(S)}{\\displaystyle O(S)}, where S is sum of all integers to be sorted, in case of small integers it can be considered to be linear. Simple pancake sort | \u2014 | n | n | {\\displaystyle \\log n}\\log n | No | Yes | Count is number of flips. Spaghetti (Poll) sort | n | n | n | {\\displaystyle n^{2}}n^{2} | Yes | Polling | This is a linear-time, analog algorithm for sorting a sequence of items, requiring O(n) stack space, and the sort is stable. This requires n parallel processors. See spaghetti sort#Analysis. Sorting network | {\\displaystyle \\log ^{2}n}\\log ^{2}n | {\\displaystyle \\log ^{2}n}\\log ^{2}n | {\\displaystyle \\log ^{2}n}\\log ^{2}n | {\\displaystyle n\\log ^{2}n}n\\log ^{2}n | Varies (stable sorting networks require more comparisons) | Yes | Order of comparisons are set in advance based on a fixed network size. Impractical for more than 32 items.[disputed \u2013 discuss] Bitonic sorter | {\\displaystyle \\log ^{2}n}\\log ^{2}n | {\\displaystyle \\log ^{2}n}\\log ^{2}n | {\\displaystyle \\log ^{2}n}\\log ^{2}n | {\\displaystyle n\\log ^{2}n}n\\log ^{2}n | No | Yes | An effective variation of Sorting networks. Bogosort | n | {\\displaystyle (n\\times n!)}{\\displaystyle (n\\times n!)} | {\\displaystyle \\infty }\\infty | 1 | No | Yes | Random shuffling. Used for example purposes only, as sorting with unbounded worst case running time. Stooge sort | {\\displaystyle n^{\\log 3/\\log 1.5}}n^{{\\log 3/\\log 1.5}} | {\\displaystyle n^{\\log 3/\\log 1.5}}n^{{\\log 3/\\log 1.5}} | {\\displaystyle n^{\\log 3/\\log 1.5}}n^{{\\log 3/\\log 1.5}} | n | No | Yes | Slower than most of the sorting algorithms (even naive ones) with a time complexity of O(nlog 3 / log 1.5 ) = O(n2.7095...). Theoretical computer scientists have detailed other sorting algorithms that provide better than O(n log n) time complexity assuming additional constraints, including: Han's algorithm, a deterministic algorithm for sorting keys from a domain of finite size, taking O(n log log n) time and O(n) space.[17] Thorup's algorithm, a randomized algorithm for sorting keys from a domain of finite size, taking O(n log log n) time and O(n) space.[18] A randomized integer sorting algorithm taking {\\displaystyle O\\left(n{\\sqrt {\\log \\log n}}\\right)}{\\displaystyle O\\left(n{\\sqrt {\\log \\log n}}\\right)} expected time and O(n) space.[19]","title":"Wwwi"},{"location":"big_data/wwwi/#comparison-sorts","text":"Name Best Average Worst Memory Stable Method Other notes Quicksort {\\displaystyle n\\log n}n\\log n variation is n {\\displaystyle n\\log n}n\\log n {\\displaystyle n^{2}}n^{2} {\\displaystyle \\log n}\\log n on average, worst case space complexity is n; Sedgewick variation is {\\displaystyle \\log n}\\log n worst case. Typical in-place sort is not stable; stable versions exist. Merge sort {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n n A hybrid block merge sort is O(1) mem. Yes Merging In-place merge sort \u2014 \u2014 {\\displaystyle n\\log ^{2}n}n\\log ^{2}n See above, for hybrid, that is {\\displaystyle n\\log n}n\\log n 1 Yes Merging Heapsort n If all keys are distinct, {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n 1 No Selection Insertion sort n {\\displaystyle n^{2}}n^{2} {\\displaystyle n^{2}}n^{2} 1 Yes Insertion O(n + d), in the worst case over sequences that have d inversions. Introsort {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n {\\displaystyle \\log n}\\log n No Partitioning & Selection Used in several STL implementations. Selection sort {\\displaystyle n^{2}}n^{2} {\\displaystyle n^{2}}n^{2} {\\displaystyle n^{2}}n^{2} 1 No Selection Stable with {\\displaystyle O(n)}O(n) extra space or when using linked lists.[9] Timsort n {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n n Yes Insertion & Merging Makes n comparisons when the data is already sorted or reverse sorted. Cubesort n {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n n Yes Insertion Makes n comparisons when the data is already sorted or reverse sorted. Shell sort {\\displaystyle n\\log n}n\\log n Depends on gap sequence Depends on gap sequence; best known is {\\displaystyle n^{4/3}}{\\displaystyle n^{4/3}} 1 No Insertion Bubble sort n {\\displaystyle n^{2}}n^{2} {\\displaystyle n^{2}}n^{2} 1 Yes Exchanging Tiny code size. Binary tree sort {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n(balanced) n Yes Insertion When using a self-balancing binary search tree. Cycle sort {\\displaystyle n^{2}}n^{2} {\\displaystyle n^{2}}n^{2} {\\displaystyle n^{2}}n^{2} 1 No Insertion In-place with theoretically optimal number of writes. Library sort n {\\displaystyle n\\log n}n\\log n {\\displaystyle n^{2}}n^{2} n Yes Insertion Patience sorting n \u2014 {\\displaystyle n\\log n}n\\log n n No Insertion & Selection Finds all the longest increasing subsequences in O(n log n). Smoothsort n {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n 1 No Selection An adaptive variant of heapsort based upon the Leonardo sequence rather than a traditional binary heap. Strand sort n {\\displaystyle n^{2}}n^{2} {\\displaystyle n^{2}}n^{2} n Yes Selection Tournament sort {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n n[10] No Selection Variation of Heap Sort. Cocktail sort n {\\displaystyle n^{2}}n^{2} {\\displaystyle n^{2}}n^{2} 1 Yes Exchanging Comb sort {\\displaystyle n\\log n}n\\log n {\\displaystyle n^{2}}n^{2} {\\displaystyle n^{2}}n^{2} 1 No Exchanging Faster than bubble sort on average. Gnome sort n {\\displaystyle n^{2}}n^{2} {\\displaystyle n^{2}}n^{2} 1 Yes Exchanging Tiny code size. UnShuffle Sort[11] n kn kn In-place for linked lists. n * sizeof(link) for array. n+1 for array? No Distribution and Merge No exchanges are performed. The parameter k is proportional to the entropy in the input. k = 1 for ordered or reverse ordered input. Franceschini's method[12] \u2014 {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n 1 Yes ? Block sort n {\\displaystyle n\\log n}n\\log n {\\displaystyle n\\log n}n\\log n 1 Yes Insertion & Merging Combine a block-based {\\displaystyle O(n)}O(n) in-place merge algorithm[13] with a bottom-up merge sort. Odd\u2013even sort n {\\displaystyle n^{2}}n^{2} {\\displaystyle n^{2}}n^{2} 1 Yes Exchanging Can be run on parallel processors easily. Curve sort n {\\displaystyle n^{2}}n^{2} {\\displaystyle n^{2}}n^{2} {\\displaystyle \\Omega (1)\\cap {\\mathcal {O}}(n)}{\\displaystyle \\Omega (1)\\cap {\\mathcal {O}}(n)} Yes Insertion & counting Adapts to the smoothness of data. Non-comparison sorts The following table describes integer sorting algorithms and other sorting algorithms that are not comparison sorts. As such, they are not limited to \u03a9(n log n)[citation needed]. Complexities below assume n items to be sorted, with keys of size k, digit size d, and r the range of numbers to be sorted. Many of them are based on the assumption that the key size is large enough that all entries have unique key values, and hence that n \u226a 2k, where \u226a means \"much less than\". In the unit-cost random access machine model, algorithms with running time of {\\displaystyle \\scriptstyle n\\cdot {\\frac {k}{d}}}{\\displaystyle \\scriptstyle n\\cdot {\\frac {k}{d}}}, such as radix sort, still take time proportional to \u0398(n log n), because n is limited to be not more than {\\displaystyle 2^{\\frac {k}{d}}}2^{\\frac {k}{d}}, and a larger number of elements to sort would require a bigger k in order to store them in the memory.[14] Non-comparison sorts Name | Best | Average | Worst | Memory | Stable | n \u226a 2k | Notes Pigeonhole sort | \u2014 | {\\displaystyle n+2^{k}}n+2^{k} | {\\displaystyle n+2^{k}}n+2^{k} | {\\displaystyle 2^{k}}2^{k} | Yes | Yes | Bucket sort (uniform keys) | \u2014 | {\\displaystyle n+k}n+k | {\\displaystyle n^{2}\\cdot k}n^{2}\\cdot k | {\\displaystyle n\\cdot k}n\\cdot k | Yes | No | Assumes uniform distribution of elements from the domain in the array.[15] Bucket sort (integer keys) | \u2014 | {\\displaystyle n+r}n+r | {\\displaystyle n+r}n+r | {\\displaystyle n+r}n+r | Yes | Yes | If r is {\\displaystyle O(n)}O(n), then average time complexity is {\\displaystyle O(n)}O(n).[16] Counting sort | \u2014 | {\\displaystyle n+r}n+r | {\\displaystyle n+r}n+r | {\\displaystyle n+r}n+r | Yes | Yes | If r is {\\displaystyle O(n)}O(n), then average time complexity is {\\displaystyle O(n)}O(n).[15] LSD Radix Sort | \u2014 | {\\displaystyle n\\cdot {\\frac {k}{d}}}n\\cdot {\\frac {k}{d}} | {\\displaystyle n\\cdot {\\frac {k}{d}}}n\\cdot {\\frac {k}{d}} | {\\displaystyle n+2^{d}}n+2^{d} | Yes | No | ,[15][16] {\\displaystyle {\\frac {k}{d}}}{\\frac {k}{d}} recursion levels, 2d for count array. MSD Radix Sort | \u2014 | {\\displaystyle n\\cdot {\\frac {k}{d}}}n\\cdot {\\frac {k}{d}} | {\\displaystyle n\\cdot {\\frac {k}{d}}}n\\cdot {\\frac {k}{d}} | {\\displaystyle n+2^{d}}n+2^{d} | Yes | No | Stable version uses an external array of size n to hold all of the bins. MSD Radix Sort (in-place) | \u2014 | {\\displaystyle n\\cdot {\\frac {k}{d}}}n\\cdot {\\frac {k}{d}} | {\\displaystyle n\\cdot {\\frac {k}{d}}}n\\cdot {\\frac {k}{d}} | {\\displaystyle 2^{d}}2^{d} | No | No | d=1 for in-place, {\\displaystyle k/1}{\\displaystyle k/1} recursion levels, no count array. Spreadsort | n | {\\displaystyle n\\cdot {\\frac {k}{d}}}n\\cdot {\\frac {k}{d}} | {\\displaystyle n\\cdot \\left({{\\frac {k}{s}}+d}\\right)}n\\cdot \\left({{\\frac {k}{s}}+d}\\right) | {\\displaystyle {\\frac {k}{d}}\\cdot 2^{d}}{\\frac {k}{d}}\\cdot 2^{d} | No | No | Asymptotic are based on the assumption that n \u226a 2k, but the algorithm does not require this. Burstsort | \u2014 | {\\displaystyle n\\cdot {\\frac {k}{d}}}n\\cdot {\\frac {k}{d}} | {\\displaystyle n\\cdot {\\frac {k}{d}}}n\\cdot {\\frac {k}{d}} | {\\displaystyle n\\cdot {\\frac {k}{d}}}n\\cdot {\\frac {k}{d}} | No | No | Has better constant factor than radix sort for sorting strings. Though relies somewhat on specifics of commonly encountered strings. Flashsort | n | {\\displaystyle n+r}n+r | {\\displaystyle n^{2}}n^{2} | n | No | No | Requires uniform distribution of elements from the domain in the array to run in linear time. If distribution is extremely skewed then it can go quadratic if underlying sort is quadratic (it is usually an insertion sort). In-place version is not stable. Postman sort | \u2014 | {\\displaystyle n\\cdot {\\frac {k}{d}}}n\\cdot {\\frac {k}{d}} | {\\displaystyle n\\cdot {\\frac {k}{d}}}n\\cdot {\\frac {k}{d}} | {\\displaystyle n+2^{d}}n+2^{d} | \u2014 | No | A variation of bucket sort, which works very similar to MSD Radix Sort. Specific to post service needs. Samplesort can be used to parallelize any of the non-comparison sorts, by efficiently distributing data into several buckets and then passing down sorting to several processors, with no need to merge as buckets are already sorted between each other. Others Some algorithms are slow compared to those discussed above, such as the bogosort with unbounded run time and the stooge sort which has O(n2.7) run time. These sorts are usually described for educational purposes in order to demonstrate how run time of algorithms is estimated. The following table describes some sorting algorithms that are impractical for real-life use in traditional software contexts due to extremely poor performance or specialized hardware requirements. Name | Best | Average | Worst | Memory | Stable | Comparison | Other notes Bead sort | n | S | S | {\\displaystyle n^{2}}n^{2} | N/A | No | Works only with positive integers. Requires specialized hardware for it to run in guaranteed {\\displaystyle O(n)}O(n) time. There is a possibility for software implementation, but running time will be {\\displaystyle O(S)}{\\displaystyle O(S)}, where S is sum of all integers to be sorted, in case of small integers it can be considered to be linear. Simple pancake sort | \u2014 | n | n | {\\displaystyle \\log n}\\log n | No | Yes | Count is number of flips. Spaghetti (Poll) sort | n | n | n | {\\displaystyle n^{2}}n^{2} | Yes | Polling | This is a linear-time, analog algorithm for sorting a sequence of items, requiring O(n) stack space, and the sort is stable. This requires n parallel processors. See spaghetti sort#Analysis. Sorting network | {\\displaystyle \\log ^{2}n}\\log ^{2}n | {\\displaystyle \\log ^{2}n}\\log ^{2}n | {\\displaystyle \\log ^{2}n}\\log ^{2}n | {\\displaystyle n\\log ^{2}n}n\\log ^{2}n | Varies (stable sorting networks require more comparisons) | Yes | Order of comparisons are set in advance based on a fixed network size. Impractical for more than 32 items.[disputed \u2013 discuss] Bitonic sorter | {\\displaystyle \\log ^{2}n}\\log ^{2}n | {\\displaystyle \\log ^{2}n}\\log ^{2}n | {\\displaystyle \\log ^{2}n}\\log ^{2}n | {\\displaystyle n\\log ^{2}n}n\\log ^{2}n | No | Yes | An effective variation of Sorting networks. Bogosort | n | {\\displaystyle (n\\times n!)}{\\displaystyle (n\\times n!)} | {\\displaystyle \\infty }\\infty | 1 | No | Yes | Random shuffling. Used for example purposes only, as sorting with unbounded worst case running time. Stooge sort | {\\displaystyle n^{\\log 3/\\log 1.5}}n^{{\\log 3/\\log 1.5}} | {\\displaystyle n^{\\log 3/\\log 1.5}}n^{{\\log 3/\\log 1.5}} | {\\displaystyle n^{\\log 3/\\log 1.5}}n^{{\\log 3/\\log 1.5}} | n | No | Yes | Slower than most of the sorting algorithms (even naive ones) with a time complexity of O(nlog 3 / log 1.5 ) = O(n2.7095...). Theoretical computer scientists have detailed other sorting algorithms that provide better than O(n log n) time complexity assuming additional constraints, including: Han's algorithm, a deterministic algorithm for sorting keys from a domain of finite size, taking O(n log log n) time and O(n) space.[17] Thorup's algorithm, a randomized algorithm for sorting keys from a domain of finite size, taking O(n log log n) time and O(n) space.[18] A randomized integer sorting algorithm taking {\\displaystyle O\\left(n{\\sqrt {\\log \\log n}}\\right)}{\\displaystyle O\\left(n{\\sqrt {\\log \\log n}}\\right)} expected time and O(n) space.[19]","title":"Comparison sorts"},{"location":"mkdocs/quickstart/","text":"For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Quickstart"},{"location":"mkdocs/quickstart/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs help - Print this help message.","title":"Commands"},{"location":"mkdocs/quickstart/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"search/","text":"Interviews HackerOne : Take home assignment today Toggle : Interview on 01/11 Ebury : Interview on 04/11 Upstack : Technical interview on 06/11","title":"Home"},{"location":"search/#interviews","text":"HackerOne : Take home assignment today Toggle : Interview on 01/11 Ebury : Interview on 04/11 Upstack : Technical interview on 06/11","title":"Interviews"},{"location":"sort/shell_sort/","text":"Python Program to Implement Shell Sort Create a generator gaps that takes the size of the list as argument and returns the next element in the sequence of gaps on each successive call. Here the gap sequence chosen is given by 2^k \u2013 1. Create a function shell_sort that takes a list as argument. For each gap returned by gaps, call insertion_sort_with_gap with gap as argument. Create a function insertion_sort_with_gap that takes a variable gap as argument. The function insertion_sort_with_gap performs insertion sort on all elements at a distance of gap from each other. Thus it performs insertion sort on the indexes k, k + gap, k + 2 gap, k + 3 gap, \u2026 of the list for all values of k. def gaps(size): # uses the gap sequence 2^k - 1: 1, 3, 7, 15, 31, ... length = size.bit_length() for k in range(length - 1, 0, -1): yield 2**k - 1 def shell_sort(alist): def insertion_sort_with_gap(gap): for i in range(gap, len(alist)): temp = alist[i] j = i - gap while (j >= 0 and temp < alist[j]): alist[j + gap] = alist[j] j = j - gap alist[j + gap] = temp for g in gaps(len(alist)): insertion_sort_with_gap(g) alist = input('Enter the list of numbers: ').split() alist = [int(x) for x in alist] shell_sort(alist) print('Sorted list: ', end='') print(alist) Runtime Test Cases Case 1: Enter the list of numbers: 5 2 3 1 10 Sorted list: 1, 2, 3, 5, 10 Case 2: Enter the list of numbers: 7 6 5 4 Sorted list: 4, 5, 6, 7 Case 3: Enter the list of numbers: 2 Sorted list: 2","title":"Shell Sort"},{"location":"sort/shell_sort/#python-program-to-implement-shell-sort","text":"Create a generator gaps that takes the size of the list as argument and returns the next element in the sequence of gaps on each successive call. Here the gap sequence chosen is given by 2^k \u2013 1. Create a function shell_sort that takes a list as argument. For each gap returned by gaps, call insertion_sort_with_gap with gap as argument. Create a function insertion_sort_with_gap that takes a variable gap as argument. The function insertion_sort_with_gap performs insertion sort on all elements at a distance of gap from each other. Thus it performs insertion sort on the indexes k, k + gap, k + 2 gap, k + 3 gap, \u2026 of the list for all values of k. def gaps(size): # uses the gap sequence 2^k - 1: 1, 3, 7, 15, 31, ... length = size.bit_length() for k in range(length - 1, 0, -1): yield 2**k - 1 def shell_sort(alist): def insertion_sort_with_gap(gap): for i in range(gap, len(alist)): temp = alist[i] j = i - gap while (j >= 0 and temp < alist[j]): alist[j + gap] = alist[j] j = j - gap alist[j + gap] = temp for g in gaps(len(alist)): insertion_sort_with_gap(g) alist = input('Enter the list of numbers: ').split() alist = [int(x) for x in alist] shell_sort(alist) print('Sorted list: ', end='') print(alist) Runtime Test Cases Case 1: Enter the list of numbers: 5 2 3 1 10 Sorted list: 1, 2, 3, 5, 10 Case 2: Enter the list of numbers: 7 6 5 4 Sorted list: 4, 5, 6, 7 Case 3: Enter the list of numbers: 2 Sorted list: 2","title":"Python Program to Implement Shell Sort"},{"location":"sort/medium/sorting/","text":"From medium Sorting overview Sorting is a skill that every software engineer and developer needs some knowledge of. Not only to pass coding interviews but as a general understanding of programming itself. The different sorting algorithms are a perfect showcase of how algorithm design can have such a strong effect on program complexity, speed, and efficiency. Let\u2019s take a tour of the top 6 sorting algorithms and see how we can implement them in Python! Algorithm comparison Bubble sort Bubble sort is the one usually taught in introductory CS classes since it clearly demonstrates how sort works while being simple and easy to understand. Bubble sort steps through the list and compares adjacent pairs of elements. The elements are swapped if they are in the wrong order. The pass through the unsorted portion of the list is repeated until the list is sorted. Because Bubble sort repeatedly passes through the unsorted part of the list, it has a worst case complexity of O(n\u00b2). def bubble_sort(arr): def swap(i, j): arr[i], arr[j] = arr[j], arr[i] n = len(arr) swapped = True x = -1 while swapped: swapped = False x = x + 1 for i in range(1, n-x): if arr[i - 1] > arr[i]: swap(i - 1, i) swapped = True return arr Selection sort Selection sort is also quite simple but frequently outperforms bubble sort. If you are choosing between the two, it\u2019s best to just default right to selection sort. With Selection sort, we divide our input list / array into two parts: The sublist of items already sorted The sublist of items remaining to be sorted that make up the rest of the list. We first find the smallest element in the unsorted sublist and place it at the end of the sorted sublist. Thus, we are continuously grabbing the smallest unsorted element and placing it in sorted order in the sorted sublist. This process continues iteratively until the list is fully sorted. def selection_sort(arr): for i in range(len(arr)): minimum = i for j in range(i + 1, len(arr)): # Select the smallest value if arr[j] < arr[minimum]: minimum = j # Place it at the front of the # sorted end of the array arr[minimum], arr[i] = arr[i], arr[minimum] return arr Insertion sort Insertion sort is both faster and well-arguably more simplistic than both bubble sort and selection sort. How many people sort their cards when playing a card game On each loop iteration, insertion sort removes one element from the array. It then finds the location where that element belongs within another sorted array and inserts it there. It repeats this process until no input elements remain. def insertion_sort(arr): for i in range(len(arr)): cursor = arr[i] pos = i while pos > 0 and arr[pos - 1] > cursor: # Swap the number down the list arr[pos] = arr[pos - 1] pos = pos - 1 # Break and do the final swap arr[pos] = cursor return arr Merge sort - Merge sort is a perfectly elegant example of a Divide and Conquer algorithm. - It simple uses the 2 main steps of such an algorithm: 1. Continuously divide the unsorted list until you have N sublists, - where each sublist has 1 element that is \u201cunsorted\u201d and N is the number of elements in the original array. 2. Repeatedly merge - i.e conquer the sublists together 2 at a time to produce new sorted sublists until all elements have been fully merged into a single sorted array. def merge_sort(arr): # The last array split if len(arr) <= 1: return arr mid = len(arr) // 2 # Perform merge_sort recursively on both halves left, right = merge_sort(arr[:mid]), merge_sort(arr[mid:]) # Merge each side together return merge(left, right, arr.copy()) def merge(left, right, merged): left_cursor, right_cursor = 0, 0 while left_cursor < len(left) and right_cursor < len(right): # Sort each one and place into the result if left[left_cursor] <= right[right_cursor]: merged[left_cursor+right_cursor]=left[left_cursor] left_cursor += 1 else: merged[left_cursor + right_cursor] = right[right_cursor] right_cursor += 1 for left_cursor in range(left_cursor, len(left)): merged[left_cursor + right_cursor] = left[left_cursor] for right_cursor in range(right_cursor, len(right)): merged[left_cursor + right_cursor] = right[right_cursor] return merged Quick sort Quick sort is also a divide and conquer algorithm like merge sort. Although it\u2019s a bit more complicated, in most standard implementations it performs significantly faster than merge sort and rarely reaches its worst case complexity of O(n\u00b2). It has 3 main steps: We first select an element which we will call the pivot from the array. Move all elements that are smaller than the pivot to the left of the pivot; move all elements that are larger than the pivot to the right of the pivot. This is called the partition operation. Recursively apply the above 2 steps separately to each of the sub-arrays of elements with smaller and bigger values than the last pivot. def partition(array, begin, end): pivot_idx = begin for i in range(begin+1, end+1): if array[i] <= array[begin]: pivot_idx += 1 array[i], array[pivot_idx] = array[pivot_idx], array[i] array[pivot_idx], array[begin] = array[begin], array[pivot_idx] return pivot_idx def quick_sort_recursion(array, begin, end): if begin >= end: return pivot_idx = partition(array, begin, end) quick_sort_recursion(array, begin, pivot_idx-1) quick_sort_recursion(array, pivot_idx+1, end) def quick_sort(array, begin=0, end=None): if end is None: end = len(array) - 1 return quick_sort_recursion(array, begin, end) Heap Sort This popular sorting algorithm, like the Insertion and Selection sorts, segments the list into sorted and unsorted parts. It converts the unsorted segment of the list to a Heap data structure, so that we can efficiently determine the largest element. We begin by transforming the list into a Max Heap - a Binary Tree where the biggest element is the root node. We then place that item to the end of the list. We then rebuild our Max Heap which now has one less value, placing the new largest value before the last item of the list. We iterate this process of building the heap until all nodes are removed. def heapify(nums, heap_size, root_index): # Assume the index of the largest element is the root index largest = root_index left_child = (2 * root_index) + 1 right_child = (2 * root_index) + 2 # If the left child of the root is a valid index, and the element is greater # than the current largest element, then update the largest element if left_child < heap_size and nums[left_child] > nums[largest]: largest = left_child # Do the same for the right child of the root if right_child < heap_size and nums[right_child] > nums[largest]: largest = right_child # If the largest element is no longer the root element, swap them if largest != root_index: nums[root_index], nums[largest] = nums[largest], nums[root_index] # Heapify the new root element to ensure it's the largest heapify(nums, heap_size, largest) def heap_sort(nums): n = len(nums) # Create a Max Heap from the list # The 2nd argument of range means we stop at the element before -1 i.e. # the first element of the list. # The 3rd argument of range means we iterate backwards, reducing the count # of i by 1 for i in range(n, -1, -1): heapify(nums, n, i) # Move the root of the max heap to the end of for i in range(n - 1, 0, -1): nums[i], nums[0] = nums[0], nums[i] heapify(nums, i, 0) # Verify it works random_list_of_nums = [35, 12, 43, 8, 51] heap_sort(random_list_of_nums) print(random_list_of_nums) Shell sort Create a generator gaps that takes the size of the list as argument and returns the next element in the sequence of gaps on each successive call. Here the gap sequence chosen is given by 2^k \u2013 1. Create a function shell_sort that takes a list as argument. For each gap returned by gaps, call insertion_sort_with_gap with gap as argument. Create a function insertion_sort_with_gap that takes a variable gap as argument. The function insertion_sort_with_gap performs insertion sort on all elements at a distance of gap from each other. Thus it performs insertion sort on the indexes k, k + gap, k + 2 gap, k + 3 gap, \u2026 of the list for all values of k. def gaps(size): # uses the gap sequence 2^k - 1: 1, 3, 7, 15, 31, ... length = size.bit_length() for k in range(length - 1, 0, -1): yield 2**k - 1 def shell_sort(alist): def insertion_sort_with_gap(gap): for i in range(gap, len(alist)): temp = alist[i] j = i - gap while (j >= 0 and temp < alist[j]): alist[j + gap] = alist[j] j = j - gap alist[j + gap] = temp for g in gaps(len(alist)): insertion_sort_with_gap(g) alist = input('Enter the list of numbers: ').split() alist = [int(x) for x in alist] shell_sort(alist) print('Sorted list: ', end='') print(alist)","title":"Medium"},{"location":"sort/medium/sorting/#sorting-overview","text":"Sorting is a skill that every software engineer and developer needs some knowledge of. Not only to pass coding interviews but as a general understanding of programming itself. The different sorting algorithms are a perfect showcase of how algorithm design can have such a strong effect on program complexity, speed, and efficiency. Let\u2019s take a tour of the top 6 sorting algorithms and see how we can implement them in Python!","title":"Sorting overview"},{"location":"sort/medium/sorting/#algorithm-comparison","text":"","title":"Algorithm comparison"},{"location":"sort/medium/sorting/#bubble-sort","text":"Bubble sort is the one usually taught in introductory CS classes since it clearly demonstrates how sort works while being simple and easy to understand. Bubble sort steps through the list and compares adjacent pairs of elements. The elements are swapped if they are in the wrong order. The pass through the unsorted portion of the list is repeated until the list is sorted. Because Bubble sort repeatedly passes through the unsorted part of the list, it has a worst case complexity of O(n\u00b2). def bubble_sort(arr): def swap(i, j): arr[i], arr[j] = arr[j], arr[i] n = len(arr) swapped = True x = -1 while swapped: swapped = False x = x + 1 for i in range(1, n-x): if arr[i - 1] > arr[i]: swap(i - 1, i) swapped = True return arr","title":"Bubble sort"},{"location":"sort/medium/sorting/#selection-sort","text":"Selection sort is also quite simple but frequently outperforms bubble sort. If you are choosing between the two, it\u2019s best to just default right to selection sort. With Selection sort, we divide our input list / array into two parts: The sublist of items already sorted The sublist of items remaining to be sorted that make up the rest of the list. We first find the smallest element in the unsorted sublist and place it at the end of the sorted sublist. Thus, we are continuously grabbing the smallest unsorted element and placing it in sorted order in the sorted sublist. This process continues iteratively until the list is fully sorted. def selection_sort(arr): for i in range(len(arr)): minimum = i for j in range(i + 1, len(arr)): # Select the smallest value if arr[j] < arr[minimum]: minimum = j # Place it at the front of the # sorted end of the array arr[minimum], arr[i] = arr[i], arr[minimum] return arr","title":"Selection sort"},{"location":"sort/medium/sorting/#insertion-sort","text":"Insertion sort is both faster and well-arguably more simplistic than both bubble sort and selection sort. How many people sort their cards when playing a card game On each loop iteration, insertion sort removes one element from the array. It then finds the location where that element belongs within another sorted array and inserts it there. It repeats this process until no input elements remain. def insertion_sort(arr): for i in range(len(arr)): cursor = arr[i] pos = i while pos > 0 and arr[pos - 1] > cursor: # Swap the number down the list arr[pos] = arr[pos - 1] pos = pos - 1 # Break and do the final swap arr[pos] = cursor return arr","title":"Insertion sort"},{"location":"sort/medium/sorting/#merge-sort","text":"- Merge sort is a perfectly elegant example of a Divide and Conquer algorithm. - It simple uses the 2 main steps of such an algorithm: 1. Continuously divide the unsorted list until you have N sublists, - where each sublist has 1 element that is \u201cunsorted\u201d and N is the number of elements in the original array. 2. Repeatedly merge - i.e conquer the sublists together 2 at a time to produce new sorted sublists until all elements have been fully merged into a single sorted array. def merge_sort(arr): # The last array split if len(arr) <= 1: return arr mid = len(arr) // 2 # Perform merge_sort recursively on both halves left, right = merge_sort(arr[:mid]), merge_sort(arr[mid:]) # Merge each side together return merge(left, right, arr.copy()) def merge(left, right, merged): left_cursor, right_cursor = 0, 0 while left_cursor < len(left) and right_cursor < len(right): # Sort each one and place into the result if left[left_cursor] <= right[right_cursor]: merged[left_cursor+right_cursor]=left[left_cursor] left_cursor += 1 else: merged[left_cursor + right_cursor] = right[right_cursor] right_cursor += 1 for left_cursor in range(left_cursor, len(left)): merged[left_cursor + right_cursor] = left[left_cursor] for right_cursor in range(right_cursor, len(right)): merged[left_cursor + right_cursor] = right[right_cursor] return merged","title":"Merge sort"},{"location":"sort/medium/sorting/#quick-sort","text":"Quick sort is also a divide and conquer algorithm like merge sort. Although it\u2019s a bit more complicated, in most standard implementations it performs significantly faster than merge sort and rarely reaches its worst case complexity of O(n\u00b2). It has 3 main steps: We first select an element which we will call the pivot from the array. Move all elements that are smaller than the pivot to the left of the pivot; move all elements that are larger than the pivot to the right of the pivot. This is called the partition operation. Recursively apply the above 2 steps separately to each of the sub-arrays of elements with smaller and bigger values than the last pivot. def partition(array, begin, end): pivot_idx = begin for i in range(begin+1, end+1): if array[i] <= array[begin]: pivot_idx += 1 array[i], array[pivot_idx] = array[pivot_idx], array[i] array[pivot_idx], array[begin] = array[begin], array[pivot_idx] return pivot_idx def quick_sort_recursion(array, begin, end): if begin >= end: return pivot_idx = partition(array, begin, end) quick_sort_recursion(array, begin, pivot_idx-1) quick_sort_recursion(array, pivot_idx+1, end) def quick_sort(array, begin=0, end=None): if end is None: end = len(array) - 1 return quick_sort_recursion(array, begin, end)","title":"Quick sort"},{"location":"sort/medium/sorting/#heap-sort","text":"This popular sorting algorithm, like the Insertion and Selection sorts, segments the list into sorted and unsorted parts. It converts the unsorted segment of the list to a Heap data structure, so that we can efficiently determine the largest element. We begin by transforming the list into a Max Heap - a Binary Tree where the biggest element is the root node. We then place that item to the end of the list. We then rebuild our Max Heap which now has one less value, placing the new largest value before the last item of the list. We iterate this process of building the heap until all nodes are removed. def heapify(nums, heap_size, root_index): # Assume the index of the largest element is the root index largest = root_index left_child = (2 * root_index) + 1 right_child = (2 * root_index) + 2 # If the left child of the root is a valid index, and the element is greater # than the current largest element, then update the largest element if left_child < heap_size and nums[left_child] > nums[largest]: largest = left_child # Do the same for the right child of the root if right_child < heap_size and nums[right_child] > nums[largest]: largest = right_child # If the largest element is no longer the root element, swap them if largest != root_index: nums[root_index], nums[largest] = nums[largest], nums[root_index] # Heapify the new root element to ensure it's the largest heapify(nums, heap_size, largest) def heap_sort(nums): n = len(nums) # Create a Max Heap from the list # The 2nd argument of range means we stop at the element before -1 i.e. # the first element of the list. # The 3rd argument of range means we iterate backwards, reducing the count # of i by 1 for i in range(n, -1, -1): heapify(nums, n, i) # Move the root of the max heap to the end of for i in range(n - 1, 0, -1): nums[i], nums[0] = nums[0], nums[i] heapify(nums, i, 0) # Verify it works random_list_of_nums = [35, 12, 43, 8, 51] heap_sort(random_list_of_nums) print(random_list_of_nums)","title":"Heap Sort"},{"location":"sort/medium/sorting/#shell-sort","text":"Create a generator gaps that takes the size of the list as argument and returns the next element in the sequence of gaps on each successive call. Here the gap sequence chosen is given by 2^k \u2013 1. Create a function shell_sort that takes a list as argument. For each gap returned by gaps, call insertion_sort_with_gap with gap as argument. Create a function insertion_sort_with_gap that takes a variable gap as argument. The function insertion_sort_with_gap performs insertion sort on all elements at a distance of gap from each other. Thus it performs insertion sort on the indexes k, k + gap, k + 2 gap, k + 3 gap, \u2026 of the list for all values of k. def gaps(size): # uses the gap sequence 2^k - 1: 1, 3, 7, 15, 31, ... length = size.bit_length() for k in range(length - 1, 0, -1): yield 2**k - 1 def shell_sort(alist): def insertion_sort_with_gap(gap): for i in range(gap, len(alist)): temp = alist[i] j = i - gap while (j >= 0 and temp < alist[j]): alist[j + gap] = alist[j] j = j - gap alist[j + gap] = temp for g in gaps(len(alist)): insertion_sort_with_gap(g) alist = input('Enter the list of numbers: ').split() alist = [int(x) for x in alist] shell_sort(alist) print('Sorted list: ', end='') print(alist)","title":"Shell sort"}]}